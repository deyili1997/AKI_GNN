{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is only for encounter inclusion and exclusion, that is to say, the columns should not change after processing.  \n",
    "\n",
    "Encounters Inclusion and Exclusion Criteria:\n",
    "1. Age between 18 and 90.  \n",
    "2. Exclude patients with pre-existing end stage renal disease (ESRD), dialysis procedure or renal transplantation (RRT) prior to the visit.   \n",
    "3. Exclude patients who eGFR < 15 mL/min/1.73 m^2 or baseline SCr > 3.5 mg/dL.    \n",
    "4. SCr trajectories satisfy the requirements (at least one SCr measurement every day of the 3-day observation window).  \n",
    "5. Each ecnounter's AKI onset date is the most severe AKI stage onset date.  \n",
    "6. Only hospital-acquired AKI is considered, that is onset 72h after admission.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"/home/lideyi/AKI_GNN/notebooks/utils\"))\n",
    "from common_var import raw_path, ct_names, pat_id_cols, race_mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Record Table to Track Encounter Exclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "encounter_num_df = pd.DataFrame(0, index = ['Total number of encounters', \n",
    "                                          'Age between 18 and 90',\n",
    "                                          'Patients with ESRD, dialysis and RRT excluded',\n",
    "                                          'Patients with SCr baseline or eGFR out of range excluded',\n",
    "                                          'Observation window should be complete',\n",
    "                                          ], \n",
    "                              columns = ct_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_in_encounter_num_df(screen_item, ct_names, onset_df, encounter_num_df):\n",
    "    # fill in table\n",
    "    for ct_name in ct_names:\n",
    "        ct_enc_n = len(onset_df[onset_df.CENTER_NAME == ct_name])\n",
    "        print('%s: %s %d'%(ct_name, screen_item, ct_enc_n))\n",
    "        encounter_num_df.loc[screen_item, ct_name] = ct_enc_n\n",
    "    return encounter_num_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Patient ID DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/local/50050827/ipykernel_879824/2475749687.py:1: DtypeWarning: Columns (0,1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  onset_df = pd.read_csv('/blue/yonghui.wu/lideyi/AKI_GNN/raw_data/onset_df.csv')\n"
     ]
    }
   ],
   "source": [
    "onset_df = pd.read_csv('/blue/yonghui.wu/lideyi/AKI_GNN/raw_data/onset_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type formatting\n",
    "# all patid should be string\n",
    "onset_df[pat_id_cols] = onset_df[pat_id_cols].astype(str)\n",
    "# format date columns\n",
    "date_cols = ['ADMIT_DATE', 'DISCHARGE_DATE', 'AKI1_ONSET', 'AKI2_ONSET', 'AKI3_ONSET']\n",
    "for col in date_cols:\n",
    "    onset_df[col] = pd.to_datetime(onset_df[col], format = 'mixed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KUMC: Total number of encounters 266199\n",
      "UPITT: Total number of encounters 587556\n"
     ]
    }
   ],
   "source": [
    "encounter_num_df = fill_in_encounter_num_df('Total number of encounters', ct_names, onset_df, encounter_num_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Demographics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from read_dfs import read_and_format_DEMO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEMO_df = read_and_format_DEMO(ct_names, raw_path, race_mapping)\n",
    "# format type, we need SEX and RACE for eGFR calculation, thus we do not drop them here\n",
    "DEMO_df[pat_id_cols + ['SEX', 'RACE']] = DEMO_df[pat_id_cols + ['SEX', 'RACE']].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the DataFrames on the specified columns\n",
    "onset_df = onset_df.merge(DEMO_df, on=pat_id_cols, how = 'left')\n",
    "\n",
    "# all encounter should have demographics info\n",
    "assert(onset_df['AGE'].isna().mean() == 0)\n",
    "assert(onset_df['SEX'].isna().mean() == 0)\n",
    "assert(onset_df['RACE'].isna().mean() == 0)\n",
    "\n",
    "# Filter the merged DataFrame to find rows where 'AGE' < 18 or 'AGE' > 90\n",
    "onset_df = onset_df[(onset_df['AGE'] >= 18) & (onset_df['AGE'] < 90)]\n",
    "\n",
    "# If you want to reset the index of onset_df after dropping rows\n",
    "onset_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KUMC: Age between 18 and 90 262637\n",
      "UPITT: Age between 18 and 90 551072\n"
     ]
    }
   ],
   "source": [
    "encounter_num_df = fill_in_encounter_num_df('Age between 18 and 90', ct_names, onset_df, encounter_num_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Diagnoses and Filter Encounters with ESRD, Dialysis and RRT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in the original read_and_format_DX, we have DX df merged with onset_df (patients to use MDRD in that case),\n",
    "#  however, it is too large here to be merged, thus we do it separately here\n",
    "from read_dfs import read_DX, concat_dfs_to_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ESRD_dia_RRT_codes = {\n",
    "    '9': ['585.6', '39.93','39.95','54.98', 'V45.11', '55.51','55.52','55.53','55.54','55.61','55.69', 'V42.0'],\n",
    "    \n",
    "    '10': ['N18.6','5A1D00Z','5A1D60Z','5A1D70Z','5A1D80Z','5A1D90Z', 'Z99.2', '0TY00Z0','0TY00Z1','0TY00Z2',\n",
    "           '0TY10Z0','0TY10Z1','0TY10Z2','0TB00ZZ','0TB10ZZ','0TT00ZZ','0TT10ZZ','0TT20ZZ', 'Z94.0'],\n",
    "    \n",
    "    'CH': [str(cpt) for cpt in range(90935, 91000)] + \\\n",
    "        ['50300','50320','50323','50325','50327','50328','50329','50340','50360','50365','50370','50380']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lideyi/AKI_GNN/notebooks/utils/read_dfs.py:159: DtypeWarning: Columns (9) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  DX_df = pd.read_csv(data_path + \"AKI_DX.csv\", delimiter = ',')\n",
      " 50%|█████     | 1/2 [00:37<00:37, 37.43s/it]/home/lideyi/AKI_GNN/notebooks/utils/read_dfs.py:137: DtypeWarning: Columns (9) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  DX_df = pd.read_csv(data_path + \"AKI_DX.csv\", delimiter = ',', usecols=use_cols)\n",
      "100%|██████████| 2/2 [01:10<00:00, 35.10s/it]\n"
     ]
    }
   ],
   "source": [
    "DX_dict = read_DX(ct_names, raw_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return encounters related to the input code dict\n",
    "def get_enc_by_DX_code(DX_dict: dict, pat_df: pd.DataFrame, \n",
    "                       code_dict: dict, code_types: list, pat_id_cols: list) -> dict:\n",
    "    processed_DX_dict = dict()\n",
    "    ct_missing_DX_DATE = ['UTHSCSA', 'UTSW', 'UofU']\n",
    "    \n",
    "    for ct_name, DX_df in tqdm(DX_dict.items()):\n",
    "        # format type\n",
    "        DX_df[['PATID', 'DX_TYPE', 'DX']] = DX_df[['PATID', 'DX_TYPE', 'DX']].astype(str)\n",
    "        DX_df['DX_TYPE'] = DX_df['DX_TYPE'].replace('09', '9')\n",
    "        DX_df['DX_TYPE'] = DX_df['DX_TYPE'].replace('9.0', '9')\n",
    "        DX_df['DX_TYPE'] = DX_df['DX_TYPE'].replace('10.0', '10')\n",
    "        \n",
    "        # we only care about code-related DX\n",
    "        DX_in_codes = []\n",
    "        for code_type in code_types:\n",
    "            DX_df_temp = DX_df[(DX_df.DX_TYPE == code_type) & (DX_df.DX.isin(code_dict[code_type]))]\n",
    "            DX_in_codes.append(DX_df_temp)\n",
    "            \n",
    "        DX_df = pd.concat(DX_in_codes, axis = 0)\n",
    "        \n",
    "        pat_ct_df = pat_df[pat_df.CENTER_NAME == ct_name]\n",
    "        pat_ct_df = pat_ct_df.merge(DX_df[['PATID', 'DX_DATE', 'DX', 'DX_TYPE', 'DAYS_SINCE_ADMIT']], \n",
    "                                    on = 'PATID', how = 'left')\n",
    "        \n",
    "        #drop rows do not involed in the codes\n",
    "        pat_ct_df.dropna(subset=['DX'], inplace = True)\n",
    "        \n",
    "        \n",
    "        # format time cols so that we can filter \"future\" dx later\n",
    "        if ct_name not in ct_missing_DX_DATE:\n",
    "            pat_ct_df['DX_DATE'] = pd.to_datetime(pat_ct_df['DX_DATE'], format = 'mixed')\n",
    "            pat_ct_df['DX_DATE'] = pat_ct_df['DX_DATE'].dt.strftime('%Y-%m-%d')\n",
    "            pat_ct_df['DX_DATE'] = pd.to_datetime(pat_ct_df['DX_DATE'], format = 'mixed')\n",
    "        else:\n",
    "            pat_ct_df.loc[:, 'DX_DATE'] = pat_ct_df.loc[:, 'ADMIT_DATE'] + \\\n",
    "            pd.to_timedelta(pat_ct_df.loc[:, 'DAYS_SINCE_ADMIT'], unit='D')\n",
    "\n",
    "        # require that it is \"history\", filter \"future\" dx\n",
    "        pat_ct_df = pat_ct_df[pat_ct_df.DX_DATE < pat_ct_df.ADMIT_DATE]\n",
    "        \n",
    "        # keep useful info\n",
    "        pat_ct_df = pat_ct_df[pat_id_cols]\n",
    "        processed_DX_dict[ct_name] = pat_ct_df\n",
    "        \n",
    "    return processed_DX_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:32<00:00, 16.15s/it]\n"
     ]
    }
   ],
   "source": [
    "enc_to_remove_DX_dict = get_enc_by_DX_code(DX_dict, onset_df, ESRD_dia_RRT_codes, ['9', '10'], pat_id_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_to_remove_DX_all = concat_dfs_to_one(enc_to_remove_DX_dict)\n",
    "enc_to_remove_DX_all.drop_duplicates(inplace = True)\n",
    "# remove pat_id_cols matched rows from onset_df\n",
    "merged_df = onset_df.merge(enc_to_remove_DX_all, on=pat_id_cols, how='left', indicator=True)\n",
    "onset_df = merged_df[merged_df['_merge'] == 'left_only'].drop(columns='_merge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Procedures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from read_dfs import read_procedures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]/home/lideyi/AKI_GNN/notebooks/utils/read_dfs.py:234: DtypeWarning: Columns (8) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  PX_df = pd.read_csv(data_path + \"AKI_PX.csv\", delimiter = ',', usecols = ['PATID', 'PX_DATE\"+PD.DATE_SHIFT\"', 'PX','PX_TYPE'])\n",
      "100%|██████████| 2/2 [00:45<00:00, 22.61s/it]\n"
     ]
    }
   ],
   "source": [
    "PX_dict = read_procedures(ct_names, raw_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return encounters related to the input code dict\n",
    "def get_enc_by_PX_code(PX_dict, pat_df, code_dict, code_types, pat_id_cols):\n",
    "    processed_PX_dict = dict()\n",
    "    \n",
    "    for ct_name, PX_df in tqdm(PX_dict.items()):\n",
    "        # format type\n",
    "        PX_df[['PATID', 'PX_TYPE', 'PX']] = PX_df[['PATID', 'PX_TYPE', 'PX']].astype(str)\n",
    "        PX_df['PX_TYPE'] = PX_df['PX_TYPE'].replace('09', '9')\n",
    "        PX_df['PX_TYPE'] = PX_df['PX_TYPE'].replace('9.0', '9')\n",
    "        PX_df['PX_TYPE'] = PX_df['PX_TYPE'].replace('10.0', '10')\n",
    "        \n",
    "        # we only care about code-related PX, after that we format time\n",
    "        PX_in_codes = []\n",
    "        for code_type in code_types:\n",
    "            PX_df_temp = PX_df[(PX_df.PX_TYPE == code_type) & (PX_df.PX.isin(code_dict[code_type]))]\n",
    "            PX_in_codes.append(PX_df_temp)\n",
    "            \n",
    "        PX_df = pd.concat(PX_in_codes, axis = 0)\n",
    "        \n",
    "        pat_ct_df = pat_df[pat_df.CENTER_NAME == ct_name]\n",
    "        pat_ct_df = pat_ct_df.merge(PX_df, on = 'PATID', how = 'left')\n",
    "        \n",
    "        #drop rows do not involed in the codes\n",
    "        pat_ct_df.dropna(subset=['PX'], inplace = True)\n",
    "        \n",
    "        \n",
    "        # format time cols so that we can filter \"future\" dx later\n",
    "        pat_ct_df['PX_DATE'] = pd.to_datetime(pat_ct_df['PX_DATE'], format = 'mixed')\n",
    "        pat_ct_df['PX_DATE'] = pat_ct_df['PX_DATE'].dt.strftime('%Y-%m-%d')\n",
    "        pat_ct_df['PX_DATE'] = pd.to_datetime(pat_ct_df['PX_DATE'], format = 'mixed')\n",
    "\n",
    "\n",
    "        # require that it is \"history\", filter \"future\" px\n",
    "        pat_ct_df = pat_ct_df[pat_ct_df.PX_DATE < pat_ct_df.ADMIT_DATE]\n",
    "        \n",
    "        # keep useful info\n",
    "        pat_ct_df = pat_ct_df[pat_id_cols]\n",
    "        processed_PX_dict[ct_name] = pat_ct_df\n",
    "        \n",
    "    return processed_PX_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:28<00:00, 14.47s/it]\n"
     ]
    }
   ],
   "source": [
    "enc_to_remove_PX_dict = get_enc_by_PX_code(PX_dict, onset_df, ESRD_dia_RRT_codes, ['9', '10', 'CH'], pat_id_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_to_remove_PX_all = concat_dfs_to_one(enc_to_remove_PX_dict)\n",
    "enc_to_remove_PX_all.drop_duplicates(inplace = True)\n",
    "# remove pat_id_cols matched rows from onset_df\n",
    "merged_df = onset_df.merge(enc_to_remove_PX_all, on=pat_id_cols, how='left', indicator=True)\n",
    "onset_df = merged_df[merged_df['_merge'] == 'left_only'].drop(columns='_merge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KUMC: Patients with ESRD, dialysis and RRT excluded 262637\n",
      "UPITT: Patients with ESRD, dialysis and RRT excluded 551072\n"
     ]
    }
   ],
   "source": [
    "encounter_num_df = fill_in_encounter_num_df('Patients with ESRD, dialysis and RRT excluded', ct_names, onset_df, encounter_num_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute eGFR and Filtered by SCr Baseline and eGFR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate eGFR, based on SCr baseline\n",
    "def calculate_ckd_epi(row):\n",
    "    age = row['AGE']\n",
    "    gender = row['SEX']\n",
    "    race = row['RACE']\n",
    "    SCr = row['BASELINE_SCR']\n",
    "    \n",
    "    # Constants for the CKD-EPI formula\n",
    "    k = 0.7 if gender == 'F' else 0.9\n",
    "    alpha = -0.329 if gender == 'F' else -0.411\n",
    "    \n",
    "    # Calculate the eGFR\n",
    "    min_term = min(SCr / k, 1) ** alpha\n",
    "    max_term = max(SCr / k, 1) ** -1.209\n",
    "    age_term = 0.993 ** age\n",
    "    \n",
    "    # Gender and ethnicity adjustments\n",
    "    gender_term = 1.018 if gender == 'F' else 1\n",
    "    african_american_term = 1.159 if race == \"Black\" else 1\n",
    "    \n",
    "    eGFR = 141 * min_term * max_term * age_term * gender_term * african_american_term\n",
    "    \n",
    "    return eGFR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "onset_df['EGFR'] = onset_df.apply(calculate_ckd_epi, axis = 1)\n",
    "# Patients with SCr baseline > 3.5 mg/dL or eGFR < 15 mL/min/1.73 m^2 excluded \n",
    "onset_df = onset_df[(onset_df.EGFR > 15) & (onset_df.BASELINE_SCR < 3.5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KUMC: Patients with SCr baseline or eGFR out of range excluded 262637\n",
      "UPITT: Patients with SCr baseline or eGFR out of range excluded 551072\n"
     ]
    }
   ],
   "source": [
    "encounter_num_df = fill_in_encounter_num_df('Patients with SCr baseline or eGFR out of range excluded', ct_names, onset_df, encounter_num_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing and Sanity Check Before Merging with SCr DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Now we need to establish the prediction point for both AKI and non-AKI encounters for AKI encounters, we need to find the most severe AKI stage onset date, and the prediction point is just 24h before the onset date.  \n",
    "2. For non-AKI encounters, we need to find the middle SCr measurement date, and the prediction point is just 24h before the last SCr measurement date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the prediction target column, non-AKI = 0, AKI-1 = 1, AKI-2 = 2, AKI-3 = 3\n",
    "def set_AKI_target(row):\n",
    "    if pd.notna(row['AKI3_ONSET']):\n",
    "        return 3\n",
    "    elif pd.notna(row['AKI2_ONSET']):\n",
    "        return 2\n",
    "    elif pd.notna(row['AKI1_ONSET']):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "onset_df['AKI_TARGET'] = onset_df.apply(set_AKI_target, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AKI_TARGET\n",
       "0    0.768563\n",
       "1    0.160421\n",
       "2    0.045809\n",
       "3    0.025207\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the percentage of each AKI stage\n",
    "AKI_stage_percentage = onset_df['AKI_TARGET'].value_counts(normalize=True).sort_index()\n",
    "AKI_stage_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the most severe AKI stage onset date for each encounter\n",
    "def set_severe_AKI_onset_date(row):\n",
    "    if pd.notna(row['AKI3_ONSET']):\n",
    "        return row['AKI3_ONSET']\n",
    "    elif pd.notna(row['AKI2_ONSET']):\n",
    "        return row['AKI2_ONSET']\n",
    "    elif pd.notna(row['AKI1_ONSET']):\n",
    "        return row['AKI1_ONSET']\n",
    "    else:\n",
    "        return pd.NaT\n",
    "\n",
    "onset_df['AKI_ONSET_DATE'] = onset_df.apply(set_severe_AKI_onset_date, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from read_dfs import dup_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of rows before dropping dups:  813709\n",
      "# of rows after dropping dups:  813709\n"
     ]
    }
   ],
   "source": [
    "# dups check\n",
    "dup_check(onset_df, pat_id_cols)\n",
    "# assert AKI_ONSET_DATE between ADMIT_DATE and DISCHARGE_DATE\n",
    "AKI_onset_encounters = onset_df[onset_df.AKI_ONSET_DATE.notna()]\n",
    "assert (AKI_onset_encounters['AKI_ONSET_DATE'] >= AKI_onset_encounters['ADMIT_DATE']).all()\n",
    "assert (AKI_onset_encounters['AKI_ONSET_DATE'] <= AKI_onset_encounters['DISCHARGE_DATE']).all()\n",
    "#each center row number above zero\n",
    "for ct_name in ct_names:\n",
    "    assert(len(onset_df[onset_df.CENTER_NAME == ct_name]) > 0)\n",
    "# check two label columns match with each other\n",
    "assert onset_df.AKI_ONSET_LABEL.sum() == len(onset_df[onset_df.AKI_TARGET > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read SCr DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from read_dfs import read_and_format_SCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:40<00:00, 20.43s/it]\n",
      "100%|██████████| 2/2 [05:12<00:00, 156.43s/it]\n"
     ]
    }
   ],
   "source": [
    "SCR_df = read_and_format_SCR(ct_names, raw_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we only care about non-AKI encounters here since we will use 24h (deprecated. now 0h) \n",
    "# before the middle SCr measurement as prediciton points\n",
    "non_AKI_df = onset_df[onset_df.AKI_ONSET_LABEL == False].copy(deep = True)\n",
    "# merge on CENTER_NAME and PATID, then filtered by ADMIT_DATE and DISCHARGE_DATE\n",
    "non_AKI_SCR_df = non_AKI_df.merge(SCR_df[['CENTER_NAME', 'PATID', 'SPECIMEN_DATE', 'RESULT_NUM']], \n",
    "                                  on = ['CENTER_NAME', 'PATID'], how = 'left')\n",
    "non_AKI_SCR_df = non_AKI_SCR_df[(non_AKI_SCR_df.SPECIMEN_DATE >= non_AKI_SCR_df.ADMIT_DATE) & \n",
    "                                (non_AKI_SCR_df.SPECIMEN_DATE <= non_AKI_SCR_df.DISCHARGE_DATE)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 625255/625255 [01:23<00:00, 7516.09it/s] \n"
     ]
    }
   ],
   "source": [
    "# sort based on pat_id_cols and SPECIMEN_DATE\n",
    "non_AKI_SCR_df.sort_values(pat_id_cols + ['SPECIMEN_DATE'], inplace = True)\n",
    "\n",
    "# group by pat_id_cols and get the 75th row of each group\n",
    "def get_mid_percentile_row(group):\n",
    "    return group.iloc[int(len(group) * 0.50)]\n",
    "\n",
    "# group by pat_id_cols and get the middle row of each group\n",
    "non_AKI_mid_SCR_df = non_AKI_SCR_df.groupby(pat_id_cols).progress_apply(get_mid_percentile_row).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of rows before dropping dups:  625255\n",
      "# of rows after dropping dups:  625255\n",
      "Middle SCr measuresment overlap with admission date rate:  0.015121830293240358\n",
      "Middle SCr measuresment overlap with discharge date rate:  0.033018528440396315\n"
     ]
    }
   ],
   "source": [
    "# sanity check\n",
    "# we cannot check len(non_AKI_mid_SCR_df) == len(non_AKI_df) since some encounters do not \n",
    "# have SCr measurements, for those we just ignore them since they will be dropped latter\n",
    "# check if there are any duplicates in the middle SCr measurements\n",
    "assert (len(non_AKI_mid_SCR_df) <= len(non_AKI_df))\n",
    "dup_check(non_AKI_mid_SCR_df, pat_id_cols)\n",
    "# check how many encounters the middle SCr measurements were taken on the admission date\n",
    "print(\"Middle SCr measuresment overlap with admission date rate: \",\n",
    "len(non_AKI_mid_SCR_df[non_AKI_mid_SCR_df.SPECIMEN_DATE == non_AKI_mid_SCR_df.ADMIT_DATE]) / len(non_AKI_mid_SCR_df))\n",
    "# check how many encounters the middle SCr measurements were taken on the discharge date\n",
    "print(\"Middle SCr measuresment overlap with discharge date rate: \", \n",
    "len(non_AKI_mid_SCR_df[non_AKI_mid_SCR_df.SPECIMEN_DATE == non_AKI_mid_SCR_df.DISCHARGE_DATE]) / len(non_AKI_mid_SCR_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "# merge the non_AKI_mid_SCR_df back to onset_df\n",
    "# rename the SPECIMEN_DATE column first\n",
    "non_AKI_mid_SCR_df.rename(columns = {'SPECIMEN_DATE': 'MID_SCR_DATE'}, inplace = True)\n",
    "onset_df = onset_df.merge(non_AKI_mid_SCR_df[pat_id_cols + ['MID_SCR_DATE']], on = pat_id_cols, how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop those with missing AKI_ONSET_DATE and MID_SCR_DATE\n",
    "onset_df = onset_df[onset_df.AKI_ONSET_DATE.notna() | onset_df.MID_SCR_DATE.notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assert conditions for onset_df\n",
    "assert (onset_df[onset_df['AKI_ONSET_DATE'].notna()]['MID_SCR_DATE'].isna()).all(), \"If AKI_ONSET_DATE is not NaT, MID_SCR_DATE should be NaT\"\n",
    "assert (onset_df[onset_df['AKI_ONSET_DATE'].notna()]['AKI_TARGET'] > 0).all(), \"If AKI_ONSET_DATE is not NaT, AKI_TARGET should be > 0\"\n",
    "assert (onset_df[onset_df['AKI_ONSET_DATE'].notna()]['AKI_ONSET_LABEL'] == True).all(), \"If AKI_ONSET_DATE is not NaT, AKI_ONSET_LABEL should be True\"\n",
    "\n",
    "assert (onset_df[onset_df['AKI_ONSET_DATE'].isna()]['MID_SCR_DATE'].notna()).all(), \"If AKI_ONSET_DATE is NaT, MID_SCR_DATE should not be NaT\"\n",
    "assert (onset_df[onset_df['AKI_ONSET_DATE'].isna()]['AKI_TARGET'] == 0).all(), \"If AKI_ONSET_DATE is NaT, AKI_TARGET should be 0\"\n",
    "assert (onset_df[onset_df['AKI_ONSET_DATE'].isna()]['AKI_ONSET_LABEL'] == False).all(), \"If AKI_ONSET_DATE is NaT, AKI_ONSET_LABEL should be False\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PREDICTION_POINT column: the 24h (deprecated, now 0h) before the non-NaT date between AKI_ONSET_DATE and MID_SCR_DATE\n",
    "onset_df.loc[:, 'PREDICTION_POINT'] = onset_df[['AKI_ONSET_DATE', 'MID_SCR_DATE']].min(axis=1) - pd.Timedelta(days=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "ob_window_len = 2\n",
    "onset_df.loc[:, 'OBSERVATION_WINDOW_START'] = onset_df['PREDICTION_POINT'] - pd.Timedelta(days=ob_window_len - 1)\n",
    "onset_df.loc[:, 'OBSERVATION_WINDOW_TO_ADMIT'] = (onset_df['OBSERVATION_WINDOW_START'] - onset_df['ADMIT_DATE']).dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of OBSERVATION_WINDOW_TO_ADMIT < 0 when AKI-0: 1.51%\n",
      "Percentage of OBSERVATION_WINDOW_TO_ADMIT < 0 when AKI-1: 29.41%\n",
      "Percentage of OBSERVATION_WINDOW_TO_ADMIT < 0 when AKI-2: 47.04%\n",
      "Percentage of OBSERVATION_WINDOW_TO_ADMIT < 0 when AKI-3: 34.16%\n"
     ]
    }
   ],
   "source": [
    "AKI_0 = onset_df[onset_df['AKI_TARGET'] == 0]\n",
    "AKI_1 = onset_df[onset_df['AKI_TARGET'] == 1]\n",
    "AKI_2 = onset_df[onset_df['AKI_TARGET'] == 2]\n",
    "AKI_3 = onset_df[onset_df['AKI_TARGET'] == 3]\n",
    "window_pct_0 = (AKI_0['OBSERVATION_WINDOW_TO_ADMIT'] < 0).mean() * 100\n",
    "window_pct_1 = (AKI_1['OBSERVATION_WINDOW_TO_ADMIT'] < 0).mean() * 100\n",
    "window_pct_2 = (AKI_2['OBSERVATION_WINDOW_TO_ADMIT'] < 0).mean() * 100\n",
    "window_pct_3 = (AKI_3['OBSERVATION_WINDOW_TO_ADMIT'] < 0).mean() * 100\n",
    "\n",
    "print(f\"Percentage of OBSERVATION_WINDOW_TO_ADMIT < 0 when AKI-0: {window_pct_0:.2f}%\")\n",
    "print(f\"Percentage of OBSERVATION_WINDOW_TO_ADMIT < 0 when AKI-1: {window_pct_1:.2f}%\")\n",
    "print(f\"Percentage of OBSERVATION_WINDOW_TO_ADMIT < 0 when AKI-2: {window_pct_2:.2f}%\")\n",
    "print(f\"Percentage of OBSERVATION_WINDOW_TO_ADMIT < 0 when AKI-3: {window_pct_3:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we require that the observation window (48h) is complete\n",
    "onset_df = onset_df[onset_df['OBSERVATION_WINDOW_TO_ADMIT'] >= 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AKI_TARGET\n",
       "0    0.830832\n",
       "1    0.124317\n",
       "2    0.026632\n",
       "3    0.018219\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the percentage of each AKI stage\n",
    "AKI_stage_percentage = onset_df['AKI_TARGET'].value_counts(normalize=True).sort_index()\n",
    "AKI_stage_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KUMC: Observation window should be complete 244259\n",
      "UPITT: Observation window should be complete 496926\n"
     ]
    }
   ],
   "source": [
    "encounter_num_df = fill_in_encounter_num_df('Observation window should be complete', ct_names, onset_df, encounter_num_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>KUMC</th>\n",
       "      <th>UPITT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Total number of encounters</th>\n",
       "      <td>266199</td>\n",
       "      <td>587556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Age between 18 and 90</th>\n",
       "      <td>262637</td>\n",
       "      <td>551072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Patients with ESRD, dialysis and RRT excluded</th>\n",
       "      <td>262637</td>\n",
       "      <td>551072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Patients with SCr baseline or eGFR out of range excluded</th>\n",
       "      <td>262637</td>\n",
       "      <td>551072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Observation window should be complete</th>\n",
       "      <td>244259</td>\n",
       "      <td>496926</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      KUMC   UPITT\n",
       "Total number of encounters                          266199  587556\n",
       "Age between 18 and 90                               262637  551072\n",
       "Patients with ESRD, dialysis and RRT excluded       262637  551072\n",
       "Patients with SCr baseline or eGFR out of range...  262637  551072\n",
       "Observation window should be complete               244259  496926"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encounter_num_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PATID</th>\n",
       "      <th>ONSETS_ENCOUNTERID</th>\n",
       "      <th>ADMIT_DATE</th>\n",
       "      <th>DISCHARGE_DATE</th>\n",
       "      <th>CENTER_NAME</th>\n",
       "      <th>BASELINE_SCR</th>\n",
       "      <th>AKI1_ONSET</th>\n",
       "      <th>AKI2_ONSET</th>\n",
       "      <th>AKI3_ONSET</th>\n",
       "      <th>AKI_ONSET_LABEL</th>\n",
       "      <th>AGE</th>\n",
       "      <th>SEX</th>\n",
       "      <th>RACE</th>\n",
       "      <th>EGFR</th>\n",
       "      <th>AKI_TARGET</th>\n",
       "      <th>AKI_ONSET_DATE</th>\n",
       "      <th>MID_SCR_DATE</th>\n",
       "      <th>PREDICTION_POINT</th>\n",
       "      <th>OBSERVATION_WINDOW_START</th>\n",
       "      <th>OBSERVATION_WINDOW_TO_ADMIT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>611629</td>\n",
       "      <td>980022</td>\n",
       "      <td>2012-02-05</td>\n",
       "      <td>2012-02-09</td>\n",
       "      <td>KUMC</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>False</td>\n",
       "      <td>68</td>\n",
       "      <td>M</td>\n",
       "      <td>White</td>\n",
       "      <td>81.918005</td>\n",
       "      <td>0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2012-02-07</td>\n",
       "      <td>2012-02-07</td>\n",
       "      <td>2012-02-06</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>611629</td>\n",
       "      <td>810534</td>\n",
       "      <td>2012-01-31</td>\n",
       "      <td>2012-02-22</td>\n",
       "      <td>KUMC</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2012-02-05</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>True</td>\n",
       "      <td>68</td>\n",
       "      <td>M</td>\n",
       "      <td>White</td>\n",
       "      <td>76.992284</td>\n",
       "      <td>1</td>\n",
       "      <td>2012-02-05</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2012-02-05</td>\n",
       "      <td>2012-02-04</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>611651</td>\n",
       "      <td>8458359</td>\n",
       "      <td>2019-06-28</td>\n",
       "      <td>2019-07-01</td>\n",
       "      <td>KUMC</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>False</td>\n",
       "      <td>39</td>\n",
       "      <td>F</td>\n",
       "      <td>White</td>\n",
       "      <td>118.153207</td>\n",
       "      <td>0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2019-06-30</td>\n",
       "      <td>2019-06-30</td>\n",
       "      <td>2019-06-29</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>611651</td>\n",
       "      <td>6840293</td>\n",
       "      <td>2019-07-03</td>\n",
       "      <td>2019-07-31</td>\n",
       "      <td>KUMC</td>\n",
       "      <td>0.590000</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>False</td>\n",
       "      <td>39</td>\n",
       "      <td>F</td>\n",
       "      <td>White</td>\n",
       "      <td>115.455473</td>\n",
       "      <td>0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2019-07-15</td>\n",
       "      <td>2019-07-15</td>\n",
       "      <td>2019-07-14</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>611651</td>\n",
       "      <td>6833322</td>\n",
       "      <td>2019-06-15</td>\n",
       "      <td>2019-06-19</td>\n",
       "      <td>KUMC</td>\n",
       "      <td>0.492826</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>False</td>\n",
       "      <td>39</td>\n",
       "      <td>F</td>\n",
       "      <td>White</td>\n",
       "      <td>122.497892</td>\n",
       "      <td>0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2019-06-16</td>\n",
       "      <td>2019-06-16</td>\n",
       "      <td>2019-06-15</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>813704</th>\n",
       "      <td>PIT991837</td>\n",
       "      <td>5076016390310_0922</td>\n",
       "      <td>2014-09-30</td>\n",
       "      <td>2014-10-02</td>\n",
       "      <td>UPITT</td>\n",
       "      <td>0.815000</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>False</td>\n",
       "      <td>29</td>\n",
       "      <td>F</td>\n",
       "      <td>White</td>\n",
       "      <td>97.415728</td>\n",
       "      <td>0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2014-10-01</td>\n",
       "      <td>2014-10-01</td>\n",
       "      <td>2014-09-30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>813705</th>\n",
       "      <td>PIT995907</td>\n",
       "      <td>5088062911236_0112</td>\n",
       "      <td>2014-03-01</td>\n",
       "      <td>2014-03-04</td>\n",
       "      <td>UPITT</td>\n",
       "      <td>0.716667</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>False</td>\n",
       "      <td>54</td>\n",
       "      <td>F</td>\n",
       "      <td>White</td>\n",
       "      <td>95.470971</td>\n",
       "      <td>0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2014-03-02</td>\n",
       "      <td>2014-03-02</td>\n",
       "      <td>2014-03-01</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>813706</th>\n",
       "      <td>PIT995946</td>\n",
       "      <td>5080489850558_0202</td>\n",
       "      <td>2014-03-05</td>\n",
       "      <td>2014-03-07</td>\n",
       "      <td>UPITT</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>False</td>\n",
       "      <td>32</td>\n",
       "      <td>F</td>\n",
       "      <td>White</td>\n",
       "      <td>120.605840</td>\n",
       "      <td>0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2014-03-07</td>\n",
       "      <td>2014-03-07</td>\n",
       "      <td>2014-03-06</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>813707</th>\n",
       "      <td>PIT996157</td>\n",
       "      <td>5070474240494_0818</td>\n",
       "      <td>2015-03-29</td>\n",
       "      <td>2015-04-01</td>\n",
       "      <td>UPITT</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>False</td>\n",
       "      <td>50</td>\n",
       "      <td>F</td>\n",
       "      <td>White</td>\n",
       "      <td>106.280799</td>\n",
       "      <td>0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2015-03-31</td>\n",
       "      <td>2015-03-31</td>\n",
       "      <td>2015-03-30</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>813708</th>\n",
       "      <td>PIT998240</td>\n",
       "      <td>5208053659482_0708</td>\n",
       "      <td>2016-05-25</td>\n",
       "      <td>2016-05-28</td>\n",
       "      <td>UPITT</td>\n",
       "      <td>0.820000</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>False</td>\n",
       "      <td>52</td>\n",
       "      <td>F</td>\n",
       "      <td>White</td>\n",
       "      <td>82.271686</td>\n",
       "      <td>0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2016-05-28</td>\n",
       "      <td>2016-05-28</td>\n",
       "      <td>2016-05-27</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>741185 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            PATID  ONSETS_ENCOUNTERID ADMIT_DATE DISCHARGE_DATE CENTER_NAME  \\\n",
       "0          611629              980022 2012-02-05     2012-02-09        KUMC   \n",
       "1          611629              810534 2012-01-31     2012-02-22        KUMC   \n",
       "2          611651             8458359 2019-06-28     2019-07-01        KUMC   \n",
       "3          611651             6840293 2019-07-03     2019-07-31        KUMC   \n",
       "4          611651             6833322 2019-06-15     2019-06-19        KUMC   \n",
       "...           ...                 ...        ...            ...         ...   \n",
       "813704  PIT991837  5076016390310_0922 2014-09-30     2014-10-02       UPITT   \n",
       "813705  PIT995907  5088062911236_0112 2014-03-01     2014-03-04       UPITT   \n",
       "813706  PIT995946  5080489850558_0202 2014-03-05     2014-03-07       UPITT   \n",
       "813707  PIT996157  5070474240494_0818 2015-03-29     2015-04-01       UPITT   \n",
       "813708  PIT998240  5208053659482_0708 2016-05-25     2016-05-28       UPITT   \n",
       "\n",
       "        BASELINE_SCR AKI1_ONSET AKI2_ONSET AKI3_ONSET  AKI_ONSET_LABEL  AGE  \\\n",
       "0           0.950000        NaT        NaT        NaT            False   68   \n",
       "1           1.000000 2012-02-05        NaT        NaT             True   68   \n",
       "2           0.550000        NaT        NaT        NaT            False   39   \n",
       "3           0.590000        NaT        NaT        NaT            False   39   \n",
       "4           0.492826        NaT        NaT        NaT            False   39   \n",
       "...              ...        ...        ...        ...              ...  ...   \n",
       "813704      0.815000        NaT        NaT        NaT            False   29   \n",
       "813705      0.716667        NaT        NaT        NaT            False   54   \n",
       "813706      0.600000        NaT        NaT        NaT            False   32   \n",
       "813707      0.600000        NaT        NaT        NaT            False   50   \n",
       "813708      0.820000        NaT        NaT        NaT            False   52   \n",
       "\n",
       "       SEX   RACE        EGFR  AKI_TARGET AKI_ONSET_DATE MID_SCR_DATE  \\\n",
       "0        M  White   81.918005           0            NaT   2012-02-07   \n",
       "1        M  White   76.992284           1     2012-02-05          NaT   \n",
       "2        F  White  118.153207           0            NaT   2019-06-30   \n",
       "3        F  White  115.455473           0            NaT   2019-07-15   \n",
       "4        F  White  122.497892           0            NaT   2019-06-16   \n",
       "...     ..    ...         ...         ...            ...          ...   \n",
       "813704   F  White   97.415728           0            NaT   2014-10-01   \n",
       "813705   F  White   95.470971           0            NaT   2014-03-02   \n",
       "813706   F  White  120.605840           0            NaT   2014-03-07   \n",
       "813707   F  White  106.280799           0            NaT   2015-03-31   \n",
       "813708   F  White   82.271686           0            NaT   2016-05-28   \n",
       "\n",
       "       PREDICTION_POINT OBSERVATION_WINDOW_START  OBSERVATION_WINDOW_TO_ADMIT  \n",
       "0            2012-02-07               2012-02-06                            1  \n",
       "1            2012-02-05               2012-02-04                            4  \n",
       "2            2019-06-30               2019-06-29                            1  \n",
       "3            2019-07-15               2019-07-14                           11  \n",
       "4            2019-06-16               2019-06-15                            0  \n",
       "...                 ...                      ...                          ...  \n",
       "813704       2014-10-01               2014-09-30                            0  \n",
       "813705       2014-03-02               2014-03-01                            0  \n",
       "813706       2014-03-07               2014-03-06                            1  \n",
       "813707       2015-03-31               2015-03-30                            1  \n",
       "813708       2016-05-28               2016-05-27                            2  \n",
       "\n",
       "[741185 rows x 20 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onset_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns that are not needed\n",
    "onset_df_cleaned = onset_df.drop(columns = ['EGFR', 'AKI1_ONSET', 'AKI2_ONSET', 'AKI3_ONSET', 'AKI_ONSET_LABEL', \n",
    "                         'AKI_ONSET_DATE', 'MID_SCR_DATE',  'OBSERVATION_WINDOW_TO_ADMIT'])\n",
    "# reset index\n",
    "onset_df_cleaned.reset_index(drop=True, inplace=True)\n",
    "# reorder columns\n",
    "onset_df_cleaned = onset_df_cleaned[['CENTER_NAME', 'PATID', 'ONSETS_ENCOUNTERID', 'ADMIT_DATE', 'DISCHARGE_DATE', \n",
    "                                     'AGE', 'SEX', 'RACE', 'BASELINE_SCR', 'OBSERVATION_WINDOW_START','PREDICTION_POINT', 'AKI_TARGET']]\n",
    "# save the cleaned onset_df\n",
    "onset_df_cleaned.to_csv('/blue/yonghui.wu/lideyi/AKI_GNN/raw_data/onset_df_cleaned.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
