{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we construct feature space for all encounters. The feature type will include demographics information (age-continuous, sex-binary, race-binary, whether black), comorbidities (pre-selection, binary) and in-observation window features, including medications (pre-selection, binary), procedures (pre-selection, binary) and lab test results (pre-selection, continous values) as well as baseline SCr level (continuous).  \n",
    "\n",
    "For comorbidities and medications, we will add upper level ontology to enhance the features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Literature Reviews for AKI-related Comorbidities: diabetes, HIV/AIDS, CKD (stages 1-5), hypertension, chronic liver diseases, heart failure, gastrointestinal diseases. \n",
    "2. Literature Reviews for AKI-related Medications: Written in the nephrotoxical_drug_data in the common_var.py\n",
    "3. Literature Reviews for AKI-related Procedures: cardiac surgery, abdominal surgery, orthopaedic surgery, anesthesia, mechanical ventilation, contrast-enhanced CT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"/home/lideyi/AKI_GNN/notebooks/utils\"))\n",
    "from common_var import raw_path, ct_names, pat_id_cols\n",
    "from tqdm import tqdm\n",
    "# enable progress_apply\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Patient ID DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read patient id dataframe\n",
    "onset_df = pd.read_csv('/blue/yonghui.wu/lideyi/AKI_GNN/raw_data/onset_df_cleaned.csv')\n",
    "\n",
    "# format columns\n",
    "onset_df[pat_id_cols + ['SEX', 'RACE']] = onset_df[pat_id_cols + ['SEX', 'RACE']].astype(str)\n",
    "date_cols = ['ADMIT_DATE', 'DISCHARGE_DATE', 'OBSERVATION_WINDOW_START', 'PREDICTION_POINT']\n",
    "for col in date_cols:\n",
    "    onset_df[col] = pd.to_datetime(onset_df[col]).dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since we already have demographic information in the onset_df\n",
    "# we can directly translate them into features\n",
    "# Convert SEX column to binary\n",
    "onset_df['SEX'] = onset_df['SEX'].apply(lambda x: 1 if x == 'M' else (0 if x == 'F' else np.random.randint(0, 2)))\n",
    "# For RACE, label Black as 1 and Others as 0\n",
    "onset_df['RACE'] = onset_df['RACE'].apply(lambda x: 1 if x == 'Black' else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Medications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: https://www.aafp.org/pubs/afp/issues/2008/0915/p743.html. We organize a dictionar, with key being a drug name, values being a dictionary, containing drug class and mechanisms and RXCUI (SDC)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Centers with bad medications (high missing rates): MCW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from extract_RXCUI import get_rxcui_list\n",
    "from common_var import nephrotoxical_drug_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each drug in the nephrotoxical_drug_data, we will extract the RXCUI code\n",
    "drug_with_rxcui = {}\n",
    "for drug_name, drug_property in nephrotoxical_drug_data.items():\n",
    "    rxcui_list = get_rxcui_list(drug_name)\n",
    "    drug_property['RXCUI'] = rxcui_list\n",
    "    drug_with_rxcui[drug_name] = drug_property"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# here we do not create separate functions for medication data\n",
    "# maybe need to add more centers here\n",
    "KUMC_use_cols = ['PATID', 'MEDADMIN_START_DATE\"+PD.DATE_SHIFT\"', \n",
    "            'MEDADMIN_STOP_DATE\"+PD.DATE_SHIFT\"', 'MEDADMIN_TYPE', 'MEDADMIN_CODE']\n",
    "KUMC_med = pd.read_csv('/blue/yonghui.wu/hoyinchan/Data/data2022raw/KUMC_ORCALE/raw/AKI_AMED.csv', usecols=KUMC_use_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processing the medicaion data an concat them\n",
    "med_cols_names = ['PATID', 'MEDADMIN_START_DATE', 'MEDADMIN_STOP_DATE', 'MEDADMIN_TYPE', 'MEDADMIN_CODE']\n",
    "KUMC_med.columns = med_cols_names\n",
    "# add center name column\n",
    "KUMC_med['CENTER_NAME'] = 'KUMC'\n",
    "# maybe add more centers here in the future so just keep it\n",
    "med_df = pd.concat([KUMC_med], axis=0)\n",
    "# format string columns\n",
    "med_df[['PATID', 'MEDADMIN_TYPE', 'MEDADMIN_CODE']] = med_df[['PATID', 'MEDADMIN_TYPE', 'MEDADMIN_CODE']].astype(str)\n",
    "# before we format time columns, we need to remove the rows that we do not care (not in drug_with_rxcui)\n",
    "# Create a reverse mapping from RXCUI to drug name\n",
    "rxcui_to_drug = {rxcui: drug for drug, properties in drug_with_rxcui.items() for rxcui in properties['RXCUI']}\n",
    "# Translate MEDADMIN_CODE to drug name\n",
    "med_df['DRUG_NAME'] = med_df['MEDADMIN_CODE'].map(rxcui_to_drug)\n",
    "# Drop rows that cannot be translated\n",
    "med_df = med_df.dropna(subset=['DRUG_NAME'])\n",
    "med_df.drop(columns=['MEDADMIN_CODE'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/local/51596703/ipykernel_3224769/1257268778.py:2: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  med_df['MEDADMIN_START_DATE'] = pd.to_datetime(med_df['MEDADMIN_START_DATE']).dt.date\n",
      "/scratch/local/51596703/ipykernel_3224769/1257268778.py:3: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  med_df['MEDADMIN_STOP_DATE'] = pd.to_datetime(med_df['MEDADMIN_STOP_DATE']).dt.date\n"
     ]
    }
   ],
   "source": [
    "# now we can format the time columns\n",
    "med_df['MEDADMIN_START_DATE'] = pd.to_datetime(med_df['MEDADMIN_START_DATE']).dt.date\n",
    "med_df['MEDADMIN_STOP_DATE'] = pd.to_datetime(med_df['MEDADMIN_STOP_DATE']).dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge on PATID and filter out the rows that are not in the observation windows\n",
    "onset_med_df = onset_df.merge(med_df, on=['CENTER_NAME', 'PATID'], how='left')\n",
    "onset_med_df = onset_med_df[(onset_med_df['MEDADMIN_START_DATE'] >= onset_med_df['OBSERVATION_WINDOW_START']) & (onset_med_df['MEDADMIN_START_DATE'] <= onset_med_df['PREDICTION_POINT']) | \\\n",
    "                      (onset_med_df['MEDADMIN_STOP_DATE'] >= onset_med_df['OBSERVATION_WINDOW_START']) & (onset_med_df['MEDADMIN_STOP_DATE'] <= onset_med_df['PREDICTION_POINT'])]  \n",
    "# now we can create the medication feature, that is we need to pivot the table and turn in to binary\n",
    "med_feature = onset_med_df[pat_id_cols + ['DRUG_NAME']].drop_duplicates()\n",
    "# Pivot the med_feature dataframe\n",
    "med_feature_pivot = med_feature.pivot_table(index=pat_id_cols, columns='DRUG_NAME', aggfunc='size', fill_value=0)\n",
    "# Convert the pivot table to binary (1 if the patient used the drug, 0 otherwise)\n",
    "med_feature_pivot = (med_feature_pivot > 0).astype(int)\n",
    "# Reset the index to make it a regular dataframe\n",
    "med_feature_pivot.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_and_filter(onset_df: pd.DataFrame, feature_df: pd.DataFrame, threshold: float) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Merge the onset_df with feature_df and filter out the cols that have less than threshold 1 values\n",
    "    \"\"\"\n",
    "    # we keep the original onset_df number of columns and merge the medication feature\n",
    "    onset_df_fea_num = len(onset_df.columns)\n",
    "    # merge\n",
    "    onset_df = onset_df.merge(feature_df, on = pat_id_cols, how='left')\n",
    "    # for those do not have a redcord in the observation window, we will fill them with 0\n",
    "    onset_df.fillna(0, inplace=True)\n",
    "    \n",
    "    # drop columns of medications with the rate of 1 less then 1%\n",
    "    # Calculate the threshold for 1%\n",
    "    threshold = len(onset_df) * threshold\n",
    "    # Get the columns to keep based on the threshold\n",
    "    columns_to_keep = onset_df.columns[:onset_df_fea_num].tolist() + \\\n",
    "                    [col for col in onset_df.columns[onset_df_fea_num:] if onset_df[col].sum() >= threshold]\n",
    "    # Filter the dataframe to keep only the desired columns\n",
    "    onset_df = onset_df[columns_to_keep]\n",
    "    return onset_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge med and filter out the columns that have less than 1% 1 values\n",
    "onset_df = merge_and_filter(onset_df, med_feature_pivot, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 58/58 [00:00<00:00, 4522.41it/s]\n"
     ]
    }
   ],
   "source": [
    "# then we want to augment the medication features with their nephrotoxic mechanisms\n",
    "# we will create a new column for each mechanism\n",
    "# we allow one mechanism effects can be added up\n",
    "for drug_name, drug_property in tqdm(drug_with_rxcui.items()):\n",
    "    if drug_name in onset_df.columns:\n",
    "        mechanisms = drug_property['mechanism']\n",
    "        for mech in mechanisms:\n",
    "            if mech not in onset_df.columns:\n",
    "                onset_df[mech] = 0\n",
    "            onset_df[mech] += onset_df[drug_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Lab Test Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is different from previous method, we need to merge on encounterid since the dataframe is too large\n",
    "KUMC_lab_cols = ['PATID', 'ENCOUNTERID', 'LAB_LOINC', 'SPECIMEN_DATE\"+PD.DATE_SHIFT\"', 'RESULT_NUM']\n",
    "KUMC_lab = pd.read_csv('/blue/yonghui.wu/hoyinchan/Data/data2022raw/KUMC_ORCALE/raw/AKI_LAB.csv', usecols = KUMC_lab_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process before merging, so that we will not be out of memory\n",
    "KUMC_lab.columns = ['PATID', 'ONSETS_ENCOUNTERID', 'LAB_LOINC', 'SPECIMEN_DATE', 'RESULT_NUM']\n",
    "KUMC_lab.dropna(subset=['LAB_LOINC', 'RESULT_NUM'], inplace=True)\n",
    "KUMC_lab[\"CENTER_NAME\"] = 'KUMC'\n",
    "# format column types\n",
    "KUMC_lab[['PATID', 'ONSETS_ENCOUNTERID', 'LAB_LOINC']] = KUMC_lab[['PATID', 'ONSETS_ENCOUNTERID', 'LAB_LOINC']].astype(str)\n",
    "KUMC_lab = KUMC_lab[(KUMC_lab.CENTER_NAME.isin(onset_df.CENTER_NAME)) & (KUMC_lab.PATID.isin(onset_df.PATID)) & \\\n",
    "    (KUMC_lab.ONSETS_ENCOUNTERID.isin(onset_df.ONSETS_ENCOUNTERID))]\n",
    "# format time columns\n",
    "KUMC_lab.loc[:, \"SPECIMEN_DATE\"] = pd.to_datetime(KUMC_lab[\"SPECIMEN_DATE\"], format='%d-%b-%y').dt.date\n",
    "# requrie that all the lab tests should before 2020\n",
    "KUMC_lab = KUMC_lab[KUMC_lab['SPECIMEN_DATE'] < pd.to_datetime('2020-01-01').date()]\n",
    "KUMC_lab.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start to merge\n",
    "onset_lab_df = onset_df.merge(KUMC_lab, on=pat_id_cols, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtered by observation window\n",
    "onset_lab_df = onset_lab_df[(onset_lab_df.SPECIMEN_DATE >= onset_lab_df.OBSERVATION_WINDOW_START) & (onset_lab_df.SPECIMEN_DATE <= onset_lab_df.PREDICTION_POINT)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort onset_lab_df rows by pat_id_cols, LAB_LOINC and SPECIMEN_DATE\n",
    "onset_lab_df = onset_lab_df.sort_values(by=pat_id_cols + ['LAB_LOINC', 'SPECIMEN_DATE'])\n",
    "onset_lab_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot the onset_lab_df dataframe\n",
    "lab_feature_pivot = onset_lab_df.pivot_table(index=pat_id_cols, columns='LAB_LOINC', \n",
    "                                             values='RESULT_NUM', aggfunc='last', fill_value=np.nan)\n",
    "# Reset the index to make pat_id_cols as columns\n",
    "lab_feature_pivot.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# record the original feature number before \n",
    "onset_df_fea_num = len(onset_df.columns)\n",
    "# merge the lab test feature\n",
    "onset_df = onset_df.merge(lab_feature_pivot, on = pat_id_cols, how='left')\n",
    "# drop columns of lab tests with missing rates more than 30%\n",
    "lab_missing_rates = onset_df.isnull().mean()\n",
    "lab_columns_to_drop = lab_missing_rates[lab_missing_rates > 0.3].index\n",
    "onset_df.drop(columns=lab_columns_to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "# use MICE to impute the missing values\n",
    "\n",
    "# Create the imputer object\n",
    "imputer = IterativeImputer(max_iter=1000, random_state=0)\n",
    "\n",
    "# Fit the imputer on the onset_df\n",
    "imputer.fit(onset_df.iloc[:, onset_df_fea_num:])\n",
    "\n",
    "# Transform the onset_df to fill missing values\n",
    "onset_df.iloc[:, onset_df_fea_num:] = imputer.transform(onset_df.iloc[:, onset_df_fea_num:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Procedures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Cardiac surgery (33016-33999, 34001-37799)  \n",
    "2. Abdominal surgery (49000-49084, 49180-49255, 49320-49329, 49400-49465, 49491-49659, 49900-49900, 49904-49999)  \n",
    "3. General anesthesia (00100-00222, 00300-00352, 00400-00474, 00500-00580, 00600-00670, 00700-00797, 00800-00882,00902-00952, 01112-01173, 01200-01274, 01320-01444, 01462-01522, 01610-01680, 01710-01782, 01810-01860, 01916-01942, 01951-01953, 01958-01969, 01990-01999)\n",
    "4. Contrast-enhanced CT (70841, 70460, 70487, 72126, 70491, 71260, 73201, 72129, 73701, 74177, 72132,\n",
    "70543, 70553, 70336, 72156, 73222, 71552, 73220, 72157, 73722, 74183, 73720, 72158, 72197)\n",
    "5. Mechanical ventilation (94002-94005)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "CPT_codes = {\n",
    "    \"Cardiac surgery\": [(33016, 33999), (34001, 37799)],\n",
    "    \n",
    "    \"Abdominal_surgery\": [\n",
    "        (49000, 49084), (49180, 49255), (49320, 49329), \n",
    "        (49400, 49465), (49491, 49659), (49900, 49900), (49904, 49999)\n",
    "    ],\n",
    "    \n",
    "    \"General anesthesia\": [\n",
    "        (100, 222), (300, 352), (400, 474), (500, 580), \n",
    "        (600, 670), (700, 797), (800, 882), (902, 952), \n",
    "        (1112, 1173), (1200, 1274), (1320, 1444), (1462, 1522), \n",
    "        (1610, 1680), (1710, 1782), (1810, 1860), (1916, 1942), \n",
    "        (1951, 1953), (1958, 1969), (1990, 1999)\n",
    "    ],\n",
    "    \n",
    "    \"Contrast-enhanced CT\":  [70841, 70460, 70487, 72126, 70491, 71260, 73201, 72129, 73701, 74177, 72132,\n",
    "70543, 70553, 70336, 72156, 73222, 71552, 73220, 72157, 73722, 74183, 73720, 72158, 72197],\n",
    "    \n",
    "    \"Mechanical_ventilation\": [(94002, 94005)],\n",
    "}   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/local/51596703/ipykernel_3224769/895864485.py:2: DtypeWarning: Columns (8) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  KUMC_PX_df = pd.read_csv('/blue/yonghui.wu/hoyinchan/Data/data2022raw/KUMC_ORCALE/raw/AKI_PX.csv', usecols = KUMC_PX_cols)\n"
     ]
    }
   ],
   "source": [
    "KUMC_PX_cols = ['PATID', 'PX_DATE\"+PD.DATE_SHIFT\"', 'PX']\n",
    "KUMC_PX_df = pd.read_csv('/blue/yonghui.wu/hoyinchan/Data/data2022raw/KUMC_ORCALE/raw/AKI_PX.csv', usecols = KUMC_PX_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format column names and data types\n",
    "KUMC_PX_df.columns = ['PATID', 'PX_DATE', 'PX']\n",
    "KUMC_PX_df['CENTER_NAME'] = 'KUMC'\n",
    "KUMC_PX_df[['PATID', 'PX']] = KUMC_PX_df[['PATID', 'PX']].astype(str)\n",
    "KUMC_PX_df['PX_DATE'] = pd.to_datetime(KUMC_PX_df['PX_DATE'], format = '%d-%b-%y').dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the CPT code is in the specified range\n",
    "def CPT_in_ranges(cpt_code, ranges):\n",
    "    try:\n",
    "        cpt_int = int(cpt_code)\n",
    "        for r in ranges:\n",
    "            if isinstance(r, tuple) and r[0] <= cpt_int <= r[1]:\n",
    "                return True\n",
    "        return False\n",
    "    except ValueError:  # if the CPT code is not an integer\n",
    "        return False\n",
    "\n",
    "# check if the CPT code is in the specified values\n",
    "def CPT_in_values(cpt_code, values):\n",
    "    return cpt_code in map(str, values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [01:31<00:00, 18.36s/it]\n"
     ]
    }
   ],
   "source": [
    "# filter by the CPT codes we are insterested in\n",
    "filtered_PX_rows = []\n",
    "for PX, codes in tqdm(CPT_codes.items()):\n",
    "    if PX == \"Contrast_enhanced_CT\":\n",
    "        filtered = KUMC_PX_df[KUMC_PX_df['PX'].apply(lambda x: CPT_in_values(x, codes))]\n",
    "    else:  # for other procedures, check if the CPT code is in the specified range\n",
    "        filtered = KUMC_PX_df[KUMC_PX_df['PX'].apply(lambda x: CPT_in_ranges(x, codes))]\n",
    "    filtered_PX_rows.append(filtered)\n",
    "\n",
    "# concat the filtered rows and drop duplicates\n",
    "filtered_PX_df = pd.concat(filtered_PX_rows).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the onset_df with the filtered_PX_df\n",
    "onset_PX_df = onset_df.merge(filtered_PX_df, on=['CENTER_NAME', 'PATID'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter by observation window\n",
    "onset_PX_df = onset_PX_df[(onset_PX_df['PX_DATE'] >= onset_PX_df['OBSERVATION_WINDOW_START']) & (onset_PX_df['PX_DATE'] <= onset_PX_df['PREDICTION_POINT'])]\n",
    "# pivot onset_PX_df to get the procedure feature, values as binary, index as pat_id_cols and columns as PX\n",
    "PX_feature_pivot = onset_PX_df.pivot_table(index=pat_id_cols, columns='PX', aggfunc='size', fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the pivot table to binary (1 if the patient had the procedure, 0 otherwise)\n",
    "PX_feature_pivot = (PX_feature_pivot > 0).astype(int)\n",
    "# Reset the index to make it a regular dataframe\n",
    "PX_feature_pivot.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge PX and filter out the columns that have less than 1% 1 values\n",
    "onset_df = merge_and_filter(onset_df, PX_feature_pivot, 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Comorbidities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Diabetes: {ICD9: 250, ICD10: E08-E13}.\n",
    "2. HIV/AIDS: {ICD9: 042, V08, ICD10: B20-B24}.\n",
    "3. CKD: {ICD9: 585, ICD10: N18}. \n",
    "4. Hypertensive diseases: {ICD9: 401-405, ICD10: I10-I16, I1A}. \n",
    "5. Chronic liver diseases: {ICD9: 571, ICD10: K70-K77}.\n",
    "6. Heart failure: {ICD9: 428, ICD10: I50}. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since ICD9 codes will always be different than ICD10 codes, we can mix them here\n",
    "DX_codes = {\n",
    "    'Diabetes': ['250', 'E08', 'E09', 'E10', 'E11', 'E13'],\n",
    "    'HIV': ['042', 'B20', 'B21', 'B22', 'B23', 'B24'],\n",
    "    'CKD-1': ['585.1', 'N18.1'],\n",
    "    'CKD-2': ['585.2', 'N18.2'],\n",
    "    'CKD-3': ['585.3', 'N18.3'],\n",
    "    'CKD-4': ['585.4', 'N18.4'],\n",
    "    'CKD-5': ['585.5', 'N18.5'],\n",
    "    'Hypertensive diseases': ['401', '402', '403', '404', '405', 'I10', 'I11', 'I12', 'I13', 'I15', 'I16', 'I1A'],\n",
    "    'Chronic liver diseases': ['571', 'K70', 'K71', 'K72', 'K73', 'K74', 'K76', 'K77'],\n",
    "    'Heart failure': ['428', 'I50'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "KUMC_DX_cols = ['PATID', 'DX', 'DX_DATE\"+PD.DATE_SHIFT\"']\n",
    "KUMC_DX_df = pd.read_csv('/blue/yonghui.wu/hoyinchan/Data/data2022raw/KUMC_ORCALE/raw/AKI_DX.csv', usecols = KUMC_DX_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format the dataframe\n",
    "KUMC_DX_df.columns = ['PATID', 'DX_DATE', 'DX']\n",
    "KUMC_DX_df['CENTER_NAME'] = 'KUMC'\n",
    "KUMC_DX_df[['PATID', 'DX']] = KUMC_DX_df[['PATID', 'DX']].astype(str)\n",
    "KUMC_DX_df['DX_DATE'] = pd.to_datetime(KUMC_DX_df['DX_DATE'], format = '%d-%b-%y').dt.date\n",
    "KUMC_DX_df.dropna(inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_to_disease(DX):\n",
    "    # DX_codes here is a global variable\n",
    "    for disease, codes in DX_codes.items():\n",
    "        if any(DX.startswith(code) for code in codes):\n",
    "            return disease\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/34471063 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34471063/34471063 [03:01<00:00, 189588.03it/s]\n"
     ]
    }
   ],
   "source": [
    "# map the DX to disease\n",
    "KUMC_DX_df.loc[:, 'DX_NAME'] = KUMC_DX_df['DX'].progress_apply(map_to_disease)\n",
    "# delete the rows that cannot be mapped to any disease\n",
    "filtered_DX_df = KUMC_DX_df.dropna(subset=['DX_NAME']).reset_index(drop=True)\n",
    "filtered_DX_df.drop('DX', axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the onset_df with the filtered_DX_df\n",
    "onset_DX_df = onset_df.merge(filtered_DX_df, on=['CENTER_NAME', 'PATID'], how='left')\n",
    "# filter by admission date\n",
    "onset_DX_df = onset_DX_df[onset_DX_df['DX_DATE'] < onset_DX_df['ADMIT_DATE']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pivot onset_DX_df to get the diagnosis feature, values as binary, index as pat_id_cols and columns as DX\n",
    "DX_feature_pivot = onset_DX_df.pivot_table(index=pat_id_cols, columns='DX_NAME', aggfunc='size', fill_value=0)\n",
    "# Convert the pivot table to binary (1 if the patient had the procedure, 0 otherwise)\n",
    "DX_feature_pivot = (DX_feature_pivot > 0).astype(int)\n",
    "# Reset the index to make it a regular dataframe\n",
    "DX_feature_pivot.reset_index(inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge med and filter out the columns that have less than 1% 1 values\n",
    "onset_df_full = merge_and_filter(onset_df, DX_feature_pivot, 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2009-01-14 2019-12-31\n"
     ]
    }
   ],
   "source": [
    "# drop the encouters that happened after COVID-19\n",
    "onset_df_full = onset_df_full[onset_df_full['DISCHARGE_DATE'] < pd.to_datetime('2020-01-01').date()]\n",
    "print(onset_df_full.DISCHARGE_DATE.min(), onset_df_full.DISCHARGE_DATE.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create 2 new columns to indicate test set and val set, since we are using temporal split, ADMIT_DATE\n",
    "# we want the set the val and test sets to be the last 20% of the data, respectively\n",
    "onset_df_full['TRAIN_SET'] = 0\n",
    "onset_df_full['VAL_SET'] = 0\n",
    "onset_df_full['TEST_SET'] = 0\n",
    "\n",
    "onset_df_full.loc[onset_df_full.ADMIT_DATE < pd.to_datetime('2016-01-01').date(), 'TRAIN_SET'] = 1\n",
    "onset_df_full.loc[(onset_df_full.ADMIT_DATE >= pd.to_datetime('2016-01-01').date()) & \\\n",
    "                 (onset_df_full.ADMIT_DATE < pd.to_datetime('2018-01-01').date()), 'VAL_SET'] = 1\n",
    "onset_df_full.loc[onset_df_full.ADMIT_DATE >= pd.to_datetime('2018-01-01').date(), 'TEST_SET'] = 1\n",
    "\n",
    "# check that the ones in TRAIN_SET, VAL_SET, TEST_SET cover the whole dataset\n",
    "assert (onset_df_full[['TRAIN_SET', 'VAL_SET', 'TEST_SET']].sum(axis=1) == 1).all()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time range:\n",
      "Train:  2009-01-10 2015-12-31\n",
      "Val:  2016-01-01 2017-12-31\n",
      "Test:  2018-01-01 2019-12-29\n",
      "Proportions:\n",
      "Train:  0.5482699412542081\n",
      "Val:  0.21825653293719313\n",
      "Test:  0.23347352580859868\n"
     ]
    }
   ],
   "source": [
    "print(\"Time range:\")\n",
    "print('Train: ', onset_df_full[onset_df_full.TRAIN_SET == 1].ADMIT_DATE.min(), onset_df_full[onset_df_full.TRAIN_SET == 1].ADMIT_DATE.max())\n",
    "print('Val: ', onset_df_full[onset_df_full.VAL_SET == 1].ADMIT_DATE.min(), onset_df_full[onset_df_full.VAL_SET == 1].ADMIT_DATE.max())\n",
    "print('Test: ', onset_df_full[onset_df_full.TEST_SET == 1].ADMIT_DATE.min(), onset_df_full[onset_df_full.TEST_SET == 1].ADMIT_DATE.max())\n",
    "print(\"Proportions:\")\n",
    "print('Train: ', len(onset_df_full[onset_df_full.TRAIN_SET == 1]) /len(onset_df_full))\n",
    "print('Val: ', len(onset_df_full[onset_df_full.VAL_SET == 1]) /len(onset_df_full))\n",
    "print('Test: ', len(onset_df_full[onset_df_full.TEST_SET == 1]) /len(onset_df_full))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the columns that are not needed before normalization\n",
    "# we also exclude BASELINE_SCR becasue AKI was labeled by SCR as well as some patients might not have a baseline SCR\n",
    "cols_to_drop = ['CENTER_NAME', 'PATID', 'ONSETS_ENCOUNTERID', 'BASELINE_SCR', 'ADMIT_DATE', 'DISCHARGE_DATE', \n",
    "                'OBSERVATION_WINDOW_START', 'PREDICTION_POINT']\n",
    "onset_df_full.drop(columns=cols_to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # select a portion of the onset_df_full to run pilot experiments\n",
    "# # we use random sampling here, with a percentatge of 40%\n",
    "np.random.seed(88)\n",
    "onset_df_pilot = onset_df_full.sample(frac=0.4).copy(deep = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_normalize_by_set(df: pd.DataFrame, features: list) -> pd.DataFrame:\n",
    "    train_norm = df[df['TRAIN_SET'] == 1].copy(deep = True)\n",
    "    val_norm = df[df['VAL_SET'] == 1].copy(deep = True)\n",
    "    test_norm = df[df['TEST_SET'] == 1].copy(deep = True)\n",
    "    \n",
    "    train_norm[features] = min_max_normalize(train_norm[features])\n",
    "    val_norm[features] = min_max_normalize(val_norm[features])\n",
    "    test_norm[features] = min_max_normalize(test_norm[features])\n",
    "    \n",
    "    norm_df = pd.concat([train_norm, val_norm, test_norm]).sort_index()\n",
    "    return norm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_normalize(df_subset: pd.DataFrame) -> pd.DataFrame:\n",
    "    return (df_subset - df_subset.min()) / (df_subset.max() - df_subset.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = [col for col in onset_df_full.columns if col not in ['AKI_TARGET', 'TRAIN_SET', 'VAL_SET', 'TEST_SET']]\n",
    "norm_onset_df_full = min_max_normalize_by_set(onset_df_full, feature_columns)\n",
    "norm_onset_df_pilot = min_max_normalize_by_set(onset_df_pilot, feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that the ones in TRAIN_SET, VAL_SET, TEST_SET cover the whole dataset\n",
    "assert (norm_onset_df_full[['TRAIN_SET', 'VAL_SET', 'TEST_SET']].sum(axis=1) == 1).all()\n",
    "assert (norm_onset_df_pilot[['TRAIN_SET', 'VAL_SET', 'TEST_SET']].sum(axis=1) == 1).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the normalized dataframe. This is the final dataframe we will use for training\n",
    "norm_onset_df_pilot.to_csv('/blue/yonghui.wu/lideyi/AKI_GNN/raw_data/norm_df_pilot.csv', index=False)\n",
    "norm_onset_df_full.to_csv('/blue/yonghui.wu/lideyi/AKI_GNN/raw_data/norm_df_full.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
