{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb\n",
    "import torch\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def visualize(h, color):\n",
    "    # Perform t-SNE dimensionality reduction\n",
    "    z = TSNE(n_components=2).fit_transform(h.detach().cpu().numpy())\n",
    "    \n",
    "    # Ensure `color` is a CPU tensor or NumPy array\n",
    "    if isinstance(color, torch.Tensor):\n",
    "        color = color.cpu().numpy()\n",
    "    \n",
    "    # Define color mapping and labels\n",
    "    color_mapping = ['green', 'yellow', 'orange', 'red']\n",
    "    labels = ['AKI-0', 'AKI-1', 'AKI-2', 'AKI-3']\n",
    "    \n",
    "    # Create figure\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    \n",
    "    # Plot each category with its corresponding color and label\n",
    "    for i, (c, label) in enumerate(zip(color_mapping, labels)):\n",
    "        mask = (color == i)  # Select points corresponding to the current category\n",
    "        plt.scatter(\n",
    "            z[mask, 0], z[mask, 1], \n",
    "            s=1, c=c, label=label\n",
    "        )\n",
    "    \n",
    "    # Add legend\n",
    "    plt.legend(title=\"AKI Stages\", loc='best', fontsize='large', title_fontsize='large')\n",
    "    \n",
    "    # Show plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mericli\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/lideyi/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# login wandb\n",
    "wandb.login(key=\"62d0c78e72de6dacd620fc6d13ebfecfa7ce68a1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "onset_df_pilot = pd.read_csv('/blue/yonghui.wu/lideyi/AKI_GNN/raw_data/norm_df_pilot.csv')\n",
    "onset_df_pilot['VAL_SET'] = 0\n",
    "onset_df_pilot['TRAIN_SET'] = 0\n",
    "train_val_indices = onset_df_pilot[onset_df_pilot['TEST_SET'] == 0].index\n",
    "val_indices = train_val_indices[:int(0.2 * len(train_val_indices))]\n",
    "onset_df_pilot.loc[val_indices, 'VAL_SET'] = 1\n",
    "onset_df_pilot.loc[(onset_df_pilot['VAL_SET'] == 0) & (onset_df_pilot['TEST_SET'] == 0), 'TRAIN_SET'] = 1\n",
    "assert ((onset_df_pilot['TRAIN_SET'] + onset_df_pilot['VAL_SET'] + onset_df_pilot['TEST_SET']) == 1).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build PyG Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data\n",
    "from sklearn.neighbors import kneighbors_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "feature_columns = [col for col in onset_df_pilot.columns if col not in ['AKI_TARGET', 'TRAIN_SET', 'VAL_SET', 'TEST_SET']]\n",
    "# here we try use demographics and comorbidities to connect encounters\n",
    "connect_features_columns = feature_columns[3:23] + feature_columns[30:-6]\n",
    "node_features = onset_df_pilot[feature_columns].copy(deep = True).values\n",
    "connect_features = onset_df_pilot[connect_features_columns].copy(deep = True).values\n",
    "node_labels = onset_df_pilot['AKI_TARGET'].copy(deep = True).values\n",
    "train_mask = onset_df_pilot['TRAIN_SET'].copy(deep = True).values\n",
    "val_mask = onset_df_pilot['VAL_SET'].copy(deep = True).values\n",
    "test_mask = onset_df_pilot['TEST_SET'].copy(deep = True).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate a k-NN graph (e.g., k=5), note that the returned matrix is not symmetric\n",
    "k = 5\n",
    "A = kneighbors_graph(connect_features, k, mode='connectivity', metric = 'cosine', include_self=False, n_jobs = -1).toarray()\n",
    "# make adjacent matrix symmetric\n",
    "A = A + A.T\n",
    "# Ensure binary adjacent matrix\n",
    "A = (A > 0).astype(int)\n",
    "edge_index = (torch.tensor(A) > 0).nonzero().t().contiguous()\n",
    "edge_index = edge_index.to(torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = Data(x = torch.tensor(node_features, dtype = torch.float), \n",
    "            edge_index = edge_index, y = torch.tensor(node_labels, dtype = torch.long), \n",
    "            num_classes = len(np.unique(node_labels)),\n",
    "            train_mask = torch.tensor(train_mask, dtype = torch.bool), \n",
    "            val_mask = torch.tensor(val_mask, dtype = torch.bool), \n",
    "            test_mask = torch.tensor(test_mask, dtype = torch.bool))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 67\n",
      "Number of classes: 4\n",
      "Number of nodes: 41467\n",
      "Number of edges: 318242\n",
      "Average node degree: 7.67\n",
      "Number of training nodes: 23486\n",
      "Training node label rate: 0.57\n",
      "Has isolated nodes: False\n",
      "Has self-loops: False\n",
      "Is undirected: True\n"
     ]
    }
   ],
   "source": [
    "# analyse the graph\n",
    "print(f'Number of features: {data.num_features}')\n",
    "print(f'Number of classes: {data.num_classes}')\n",
    "print(f'Number of nodes: {data.num_nodes}')\n",
    "print(f'Number of edges: {data.num_edges}')\n",
    "print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
    "print(f'Number of training nodes: {data.train_mask.sum()}')\n",
    "print(f'Training node label rate: {int(data.train_mask.sum()) / data.num_nodes:.2f}')\n",
    "print(f'Has isolated nodes: {data.has_isolated_nodes()}')\n",
    "print(f'Has self-loops: {data.has_self_loops()}')\n",
    "print(f'Is undirected: {data.is_undirected()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = data.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GATConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, input_dim, n_class, hidden_dims, dropout):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(888)\n",
    "        self.conv1 = GATConv(input_dim, hidden_dims)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        self.conv2 = GATConv(hidden_dims, hidden_dims)\n",
    "        self.conv3 = GATConv(hidden_dims, hidden_dims)\n",
    "        self.linear = torch.nn.Linear(hidden_dims, n_class)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # First GCN layer\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Second GCN layer\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Third GCN layer\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Fully connected layer for output\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing METIS partitioning...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.loader import ClusterData, ClusterLoader\n",
    "\n",
    "torch.manual_seed(888)\n",
    "cluster_data = ClusterData(data, num_parts=128)  # 1. Create subgraphs.\n",
    "data_loader = ClusterLoader(cluster_data, batch_size=32, shuffle=True)  # 2. Stochastic partioning scheme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100, Train Loss: 0.4362, Train Acc: 0.8448,                   Test Loss: 0.4068, Test Acc: 0.8545\n",
      "Epoch: 200, Train Loss: 0.4553, Train Acc: 0.8392,                   Test Loss: 0.4743, Test Acc: 0.8472\n",
      "Epoch: 300, Train Loss: 0.4125, Train Acc: 0.8472,                   Test Loss: 0.3890, Test Acc: 0.8593\n",
      "Epoch: 400, Train Loss: 0.4106, Train Acc: 0.8491,                   Test Loss: 0.3970, Test Acc: 0.8571\n",
      "Epoch: 500, Train Loss: 0.3981, Train Acc: 0.8505,                   Test Loss: 0.3898, Test Acc: 0.8591\n"
     ]
    }
   ],
   "source": [
    "model = GCN(input_dim = data.num_features, n_class = data.num_classes, hidden_dims=128, dropout=0.2).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def train():\n",
    "      model.train()\n",
    "      optimizer.zero_grad()  # Clear gradients.\n",
    "      out = model(data.x, data.edge_index)  # Perform a single forward pass.\n",
    "      loss = criterion(out[data.train_mask], data.y[data.train_mask])  # Compute the loss solely based on the training nodes.\n",
    "      loss.backward()  # Derive gradients.\n",
    "      optimizer.step()  # Update parameters based on gradients.\n",
    "      # train acc\n",
    "      pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "      train_correct = pred[data.train_mask] == data.y[data.train_mask]  # Check against ground-truth labels.\n",
    "      train_acc = int(train_correct.sum()) / int(data.train_mask.sum())  # Derive ratio of correct predictions.\n",
    "      return loss, train_acc\n",
    "\n",
    "def test():\n",
    "      model.eval()\n",
    "      out = model(data.x, data.edge_index)\n",
    "      pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "      loss = criterion(out[data.test_mask], data.y[data.test_mask])\n",
    "      test_correct = pred[data.test_mask] == data.y[data.test_mask]  # Check against ground-truth labels.\n",
    "      test_acc = int(test_correct.sum()) / int(data.test_mask.sum())  # Derive ratio of correct predictions.\n",
    "      return loss, test_acc\n",
    "\n",
    "\n",
    "for epoch in range(1, 501):\n",
    "      train_loss, train_acc = train()\n",
    "      test_loss, test_acc = test()\n",
    "      if epoch % 100 == 0:\n",
    "            print(f'Epoch: {epoch:03d}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, \\\n",
    "                  Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"/home/lideyi/AKI_GNN/notebooks/utils\"))\n",
    "from metrics import performance_per_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>AUROC</th>\n",
       "      <th>AUPRC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.898608</td>\n",
       "      <td>0.975763</td>\n",
       "      <td>0.935598</td>\n",
       "      <td>0.853378</td>\n",
       "      <td>0.959527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.420168</td>\n",
       "      <td>0.214592</td>\n",
       "      <td>0.284091</td>\n",
       "      <td>0.785551</td>\n",
       "      <td>0.316552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.262295</td>\n",
       "      <td>0.051118</td>\n",
       "      <td>0.085561</td>\n",
       "      <td>0.926495</td>\n",
       "      <td>0.273064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.535316</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.603774</td>\n",
       "      <td>0.981771</td>\n",
       "      <td>0.633693</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   precision    recall  f1-score     AUROC     AUPRC\n",
       "0   0.898608  0.975763  0.935598  0.853378  0.959527\n",
       "1   0.420168  0.214592  0.284091  0.785551  0.316552\n",
       "2   0.262295  0.051118  0.085561  0.926495  0.273064\n",
       "3   0.535316  0.692308  0.603774  0.981771  0.633693"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_prob = torch.nn.functional.softmax(model(data.x, data.edge_index), dim = 1).detach().cpu().numpy()\n",
    "y_test_pred_prob = y_pred_prob[data.test_mask.detach().cpu().numpy()]\n",
    "y_test_pred = y_test_pred_prob.argmax(axis = 1)\n",
    "y_test = data.y[data.test_mask].cpu().numpy()\n",
    "performance_per_class(y_test, y_test_pred, y_test_pred_prob)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AKI_GNN",
   "language": "python",
   "name": "aki_gnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
