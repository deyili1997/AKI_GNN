{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import wandb\n",
    "import torch\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"/home/lideyi/AKI_GNN/notebooks/utils\"))\n",
    "from metrics import performance_per_class, visualize_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mericli\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/lideyi/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# login wandb\n",
    "wandb.login(key=\"62d0c78e72de6dacd620fc6d13ebfecfa7ce68a1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "onset_df_pilot = pd.read_csv('/blue/yonghui.wu/lideyi/AKI_GNN/raw_data/norm_df_pilot.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build PyG Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data\n",
    "from sklearn.neighbors import kneighbors_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "feature_columns = [col for col in onset_df_pilot.columns if col not in ['AKI_TARGET', 'TRAIN_SET', 'VAL_SET', 'TEST_SET']]\n",
    "node_features = onset_df_pilot[feature_columns].copy(deep = True).values\n",
    "node_labels = onset_df_pilot['AKI_TARGET'].copy(deep = True).values\n",
    "train_mask = onset_df_pilot['TRAIN_SET'].copy(deep = True).values\n",
    "val_mask = onset_df_pilot['VAL_SET'].copy(deep = True).values\n",
    "test_mask = onset_df_pilot['TEST_SET'].copy(deep = True).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate a k-NN graph (e.g., k=5), note that the returned matrix is not symmetric\n",
    "k = 5\n",
    "A = kneighbors_graph(node_features, k, mode='connectivity', metric = 'cosine', include_self=False, n_jobs = -1).toarray()\n",
    "# make adjacent matrix symmetric\n",
    "A = A + A.T\n",
    "# Ensure binary adjacent matrix\n",
    "A = (A > 0).astype(int)\n",
    "edge_index = (torch.tensor(A) > 0).nonzero().t().contiguous()\n",
    "edge_index = edge_index.to(torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = Data(x = torch.tensor(node_features, dtype = torch.float), \n",
    "            edge_index = edge_index, y = torch.tensor(node_labels, dtype = torch.long), \n",
    "            num_classes = len(np.unique(node_labels)),\n",
    "            train_mask = torch.tensor(train_mask, dtype = torch.bool), \n",
    "            val_mask = torch.tensor(val_mask, dtype = torch.bool), \n",
    "            test_mask = torch.tensor(test_mask, dtype = torch.bool))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 67\n",
      "Number of classes: 4\n",
      "Number of nodes: 41467\n",
      "Number of edges: 311452\n",
      "Average node degree: 7.51\n",
      "Number of training nodes: 23486\n",
      "Training node label rate: 0.57\n",
      "Has isolated nodes: False\n",
      "Has self-loops: False\n",
      "Is undirected: True\n"
     ]
    }
   ],
   "source": [
    "# analyse the graph\n",
    "print(f'Number of features: {data.num_features}')\n",
    "print(f'Number of classes: {data.num_classes}')\n",
    "print(f'Number of nodes: {data.num_nodes}')\n",
    "print(f'Number of edges: {data.num_edges}')\n",
    "print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
    "print(f'Number of training nodes: {data.train_mask.sum()}')\n",
    "print(f'Training node label rate: {int(data.train_mask.sum()) / data.num_nodes:.2f}')\n",
    "print(f'Has isolated nodes: {data.has_isolated_nodes()}')\n",
    "print(f'Has self-loops: {data.has_self_loops()}')\n",
    "print(f'Is undirected: {data.is_undirected()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes in one batch: 10432\n",
      "Data(x=[10432, 67], y=[10432], num_classes=4, train_mask=[10432], val_mask=[10432], test_mask=[10432], edge_index=[2, 71312])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing METIS partitioning...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# turn the data into loader\n",
    "from torch_geometric.loader import ClusterData, ClusterLoader\n",
    "\n",
    "torch.manual_seed(888)\n",
    "cluster_data = ClusterData(data, num_parts=128)  # 1. Create subgraphs.\n",
    "data_loader = ClusterLoader(cluster_data, batch_size=32, shuffle=True)  # 2. Stochastic partioning scheme.\n",
    "\n",
    "for step, sub_data in enumerate(data_loader):\n",
    "    print(f'Number of nodes in one batch: {sub_data.num_nodes}')\n",
    "    print(sub_data)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GATConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, input_dim, n_class, hidden_dims, dropout):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(888)\n",
    "        self.conv1 = GATConv(input_dim, hidden_dims)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        self.conv2 = GATConv(hidden_dims, hidden_dims)\n",
    "        self.conv3 = GATConv(hidden_dims, hidden_dims)\n",
    "        self.linear = torch.nn.Linear(hidden_dims, n_class)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # First GCN layer\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Second GCN layer\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Third GCN layer\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Fully connected layer for output\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GCN(input_dim = data.num_features, n_class = data.num_classes, hidden_dims=128, dropout=0.2).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def train_epoch():\n",
    "      model.train()\n",
    "\n",
    "      for sub_data in data_loader:  # Iterate over each mini-batch.\n",
    "            sub_data = sub_data.to(device)\n",
    "            optimizer.zero_grad()  # Clear gradients.\n",
    "            out = model(sub_data.x, sub_data.edge_index)  # Perform a single forward pass.\n",
    "            loss = criterion(out[sub_data.train_mask], sub_data.y[sub_data.train_mask])  # Compute the loss solely based on the training nodes.\n",
    "            loss.backward()  # Derive gradients.\n",
    "            optimizer.step()  # Update parameters based on gradients.\n",
    "      return loss\n",
    "\n",
    "def test_epoch():\n",
    "      model.eval()  # Set the model to evaluation mode.\n",
    "      \n",
    "      # Store predictions and ground truths for each mask.\n",
    "      y_true_masks = {key: [] for key in [\"train\", \"val\"]}\n",
    "      y_pred_masks = {key: [] for key in [\"train\", \"val\"]}\n",
    "      \n",
    "      with torch.no_grad():  # Disable gradient computation for evaluation.\n",
    "            for sub_data in data_loader:  # Iterate over mini-batches.\n",
    "                  sub_data = sub_data.to(device)\n",
    "                  out = model(sub_data.x, sub_data.edge_index)  # Forward pass.\n",
    "                  y_pred = out.argmax(dim=1)  # Use the class with the highest probability.\n",
    "                  \n",
    "                  # Collect predictions and ground truths for each mask.\n",
    "                  for mask, key in zip(\n",
    "                  [sub_data.train_mask, sub_data.val_mask], \n",
    "                  [\"train\", \"val\"]):\n",
    "                        y_pred_masks[key].append(y_pred[mask].cpu())\n",
    "                        y_true_masks[key].append(sub_data.y[mask].cpu())\n",
    "      \n",
    "      # Compute F1 scores for each mask.\n",
    "      F1_scores = []\n",
    "      for key in [\"train\", \"val\"]:\n",
    "            y_true_combined = torch.cat(y_true_masks[key], dim=0).numpy()\n",
    "            y_pred_combined = torch.cat(y_pred_masks[key], dim=0).numpy()\n",
    "            F1_scores.append(\n",
    "                  f1_score(y_true_combined, y_pred_combined, average=\"macro\")\n",
    "            )\n",
    "      \n",
    "      return F1_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100, Train Loss: 0.4162, Train F1: 0.4995, Val F1: 0.4957\n",
      "Epoch: 200, Train Loss: 0.3890, Train F1: 0.5539, Val F1: 0.5020\n",
      "Epoch: 300, Train Loss: 0.3914, Train F1: 0.5301, Val F1: 0.4714\n",
      "Epoch: 400, Train Loss: 0.4398, Train F1: 0.6032, Val F1: 0.5096\n",
      "Epoch: 500, Train Loss: 0.3805, Train F1: 0.6010, Val F1: 0.5018\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 501):\n",
    "      train_loss = train_epoch()\n",
    "      train_F1, val_F1 = test_epoch()\n",
    "      if epoch % 100 == 0:\n",
    "            print(f'Epoch: {epoch:03d}, Train Loss: {train_loss:.4f}, Train F1: {train_F1:.4f}, Val F1: {val_F1:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>AUROC</th>\n",
       "      <th>AUPRC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.897743</td>\n",
       "      <td>0.963988</td>\n",
       "      <td>0.929687</td>\n",
       "      <td>0.821051</td>\n",
       "      <td>0.950081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.389359</td>\n",
       "      <td>0.230329</td>\n",
       "      <td>0.289438</td>\n",
       "      <td>0.745745</td>\n",
       "      <td>0.290939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.299435</td>\n",
       "      <td>0.169329</td>\n",
       "      <td>0.216327</td>\n",
       "      <td>0.897109</td>\n",
       "      <td>0.231233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.521472</td>\n",
       "      <td>0.408654</td>\n",
       "      <td>0.458221</td>\n",
       "      <td>0.975236</td>\n",
       "      <td>0.454065</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   precision    recall  f1-score     AUROC     AUPRC\n",
       "0   0.897743  0.963988  0.929687  0.821051  0.950081\n",
       "1   0.389359  0.230329  0.289438  0.745745  0.290939\n",
       "2   0.299435  0.169329  0.216327  0.897109  0.231233\n",
       "3   0.521472  0.408654  0.458221  0.975236  0.454065"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_pred = []\n",
    "y_test_pred_proba = []\n",
    "y_test_true = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for sub_data in data_loader:\n",
    "        sub_data = sub_data.to(device)\n",
    "        out = model(sub_data.x, sub_data.edge_index)\n",
    "        y_test_pred.append(out[sub_data.test_mask].argmax(dim=1).cpu())\n",
    "        y_test_pred_proba.append(out[sub_data.test_mask].softmax(dim=1).cpu())\n",
    "        y_test_true.append(sub_data.y[sub_data.test_mask].cpu())\n",
    "\n",
    "y_test_pred = torch.cat(y_test_pred, dim=0).numpy()\n",
    "y_test_pred_proba = torch.cat(y_test_pred_proba, dim=0).numpy()\n",
    "y_test_true = torch.cat(y_test_true, dim=0).numpy()\n",
    "performance_per_class(y_test_true, y_test_pred, y_test_pred_proba)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
