{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# show all columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import wandb\n",
    "# mute wandb outputs\n",
    "os.environ[\"WANDB_SILENT\"] = \"true\"\n",
    "import torch\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "sys.path.append(os.path.abspath(\"/home/lideyi/AKI_GNN/notebooks/utils\"))\n",
    "from metrics import performance_per_class, visualize_embeddings\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# login wandb\n",
    "wandb.login(key=\"62d0c78e72de6dacd620fc6d13ebfecfa7ce68a1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "onset_df_pilot = pd.read_csv('/blue/yonghui.wu/lideyi/AKI_GNN/raw_data/norm_df_pilot.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build PyG Data Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data\n",
    "from sklearn.neighbors import kneighbors_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = [col for col in onset_df_pilot.columns if col not in ['AKI_TARGET', 'TRAIN_SET', 'VAL_SET', 'TEST_SET']]\n",
    "node_features = onset_df_pilot[feature_columns].copy(deep = True).values\n",
    "node_labels = onset_df_pilot['AKI_TARGET'].copy(deep = True).values\n",
    "train_mask = onset_df_pilot['TRAIN_SET'].copy(deep = True).values\n",
    "val_mask = onset_df_pilot['VAL_SET'].copy(deep = True).values\n",
    "test_mask = onset_df_pilot['TEST_SET'].copy(deep = True).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a k-NN graph (e.g., k=5), note that the returned matrix is not symmetric\n",
    "k = 5\n",
    "A = kneighbors_graph(node_features, k, mode='connectivity', metric = 'cosine', include_self=False, n_jobs = -1).toarray()\n",
    "# make adjacent matrix symmetric\n",
    "A = A + A.T\n",
    "# Ensure binary adjacent matrix\n",
    "A = (A > 0).astype(int)\n",
    "edge_index = (torch.tensor(A) > 0).nonzero().t().contiguous()\n",
    "edge_index = edge_index.to(torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Data(x = torch.tensor(node_features, dtype = torch.float), \n",
    "            edge_index = edge_index, y = torch.tensor(node_labels, dtype = torch.long), \n",
    "            num_classes = len(np.unique(node_labels)),\n",
    "            train_mask = torch.tensor(train_mask, dtype = torch.bool), \n",
    "            val_mask = torch.tensor(val_mask, dtype = torch.bool), \n",
    "            test_mask = torch.tensor(test_mask, dtype = torch.bool))\n",
    "# Sorts by the destination nodes (edge_index[1]), as required for some models' aggregation.\n",
    "data = data.sort(sort_by_row=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyse the graph\n",
    "print(f'Number of features: {data.num_features}')\n",
    "print(f'Number of classes: {data.num_classes}')\n",
    "print(f'Number of nodes: {data.num_nodes}')\n",
    "print(f'Number of edges: {data.num_edges}')\n",
    "print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
    "print(f'Number of training nodes: {data.train_mask.sum()}')\n",
    "print(f'Training node label rate: {int(data.train_mask.sum()) / data.num_nodes:.2f}')\n",
    "print(f'Has isolated nodes: {data.has_isolated_nodes()}')\n",
    "print(f'Has self-loops: {data.has_self_loops()}')\n",
    "print(f'Is undirected: {data.is_undirected()}')\n",
    "print(f'Is sorted by destination nodes: {data.is_sorted(sort_by_row = False)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wandb Train Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wandb.sdk.wandb_config import Config\n",
    "# turn the data into loader\n",
    "from torch_geometric.loader import ClusterData, ClusterLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_GNN(data: Data, build_GNN_func: callable, wandb_project_name: str, parameters: dict) -> pd.DataFrame:\n",
    "    sweep_config = build_sweep_config(parameters)\n",
    "    sweep_id = wandb.sweep(sweep_config, project = wandb_project_name)\n",
    "    sweep_func = lambda: train_GNN_main(data = data, build_GNN_func = build_GNN_func, config = None)\n",
    "    wandb.agent(sweep_id, sweep_func)\n",
    "    performance = test_best_GNN(data, sweep_id, build_GNN_func)\n",
    "    return performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_sweep_config(parameters: dict) -> dict:\n",
    "    sweep_config = {\n",
    "    'method': 'grid',\n",
    "    'metric': {'name': 'val_F1', 'goal': 'maximize'},\n",
    "    'parameters': parameters,\n",
    "    }\n",
    "    return sweep_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_GNN_main(data: Data, build_GNN_func: callable, config = None) -> None:\n",
    "    # Initialize a new wandb run\n",
    "    with wandb.init(config=config):\n",
    "        config = wandb.config\n",
    "        model = build_GNN_func(config)\n",
    "        optimizer = build_optimizer(model, config.optimizer, config.lr)\n",
    "        data_loader = build_dataloader(data, config.graph_num_parts, config.batch_size)\n",
    "        train_GNN(model, config.epochs, optimizer, data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_optimizer(model: torch.nn.Module, optimizer: str, lr: float) -> torch.optim.Optimizer:\n",
    "    if optimizer == \"sgd\":\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "    elif optimizer == \"adam\":\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataloader(data: Data, graph_num_parts: int, batch_size: int) -> ClusterLoader:\n",
    "    torch.manual_seed(888)\n",
    "    cluster_data = ClusterData(data, num_parts=graph_num_parts, log = False)\n",
    "    data_loader = ClusterLoader(cluster_data, batch_size=batch_size, shuffle=True) \n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_GNN(model: torch.nn.Module, epochs: int, optimizer: torch.optim.Optimizer, data_loader: ClusterLoader, \n",
    "              log: bool = True) -> None:\n",
    "    for _ in range(epochs):\n",
    "        avg_loss_train = train_epoch(model, optimizer, data_loader)\n",
    "        train_F1, val_F1 = val_epoch(model, data_loader)\n",
    "        if log:\n",
    "            wandb.log({\"train_loss\": avg_loss_train, \"train_F1\": train_F1, \"val_F1\": val_F1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model: torch.nn.Module, optimizer: torch.optim.Optimizer, data_loader: ClusterLoader) -> tuple:\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for sub_data in data_loader:\n",
    "        sub_data = sub_data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(sub_data.x, sub_data.edge_index)\n",
    "        loss = torch.nn.functional.cross_entropy(out[sub_data.train_mask], sub_data.y[sub_data.train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_epoch(model: torch.nn.Module, data_loader: ClusterLoader) -> float:\n",
    "    model.eval()  # Set the model to evaluation mode.\n",
    "    \n",
    "    # Store predictions and ground truths for each mask.\n",
    "    y_true_masks = {key: [] for key in [\"train\", \"val\"]}\n",
    "    y_pred_masks = {key: [] for key in [\"train\", \"val\"]}\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient computation for evaluation.\n",
    "        for sub_data in data_loader:  # Iterate over mini-batches.\n",
    "                sub_data = sub_data.to(device)\n",
    "                out = model(sub_data.x, sub_data.edge_index)  # Forward pass.\n",
    "                y_pred = out.argmax(dim=1)  # Use the class with the highest probability.\n",
    "                \n",
    "                # Collect predictions and ground truths for each mask.\n",
    "                for mask, key in zip(\n",
    "                [sub_data.train_mask, sub_data.val_mask], \n",
    "                [\"train\", \"val\"]):\n",
    "                    y_pred_masks[key].append(y_pred[mask].cpu())\n",
    "                    y_true_masks[key].append(sub_data.y[mask].cpu())\n",
    "    \n",
    "    # Compute F1 scores for each mask.\n",
    "    F1_scores = []\n",
    "    for key in [\"train\", \"val\"]:\n",
    "        y_true_combined = torch.cat(y_true_masks[key], dim=0).numpy()\n",
    "        y_pred_combined = torch.cat(y_pred_masks[key], dim=0).numpy()\n",
    "        F1_scores.append(\n",
    "                f1_score(y_true_combined, y_pred_combined, average=\"macro\")\n",
    "        )\n",
    "    \n",
    "    return F1_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_best_GNN(data: Data, sweep_id: str, build_GNN_func: callable) -> pd.DataFrame:\n",
    "    best_config = fetch_best_config(sweep_id)\n",
    "    best_config = turn_config_dict_to_config(best_config)\n",
    "    data_loader = build_dataloader(data, best_config.graph_num_parts, best_config.batch_size)\n",
    "    best_GNN = build_GNN_func(best_config)\n",
    "    performance = evaluate_on_test_set(best_GNN, data_loader)\n",
    "    return performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def turn_config_dict_to_config(config_dict: dict) -> Config:\n",
    "    config = Config()\n",
    "    for key, value in config_dict.items():\n",
    "        setattr(config, key, value)\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_best_config(sweep_id: str) -> dict:\n",
    "    # Authenticate with W&B\n",
    "    api = wandb.Api()\n",
    "    sweep = api.sweep(sweep_id)\n",
    "    runs = sweep.runs\n",
    "    \n",
    "    # Find the best run\n",
    "    best_run = max(runs, key=lambda run: run.summary.get(\"val_F1\", float(\"-inf\")))\n",
    "    best_config = best_run.config\n",
    "    return best_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_on_test_set(model: torch.nn.Module, data_loader: ClusterLoader) -> pd.DataFrame:\n",
    "    y_test_pred = []\n",
    "    y_test_pred_proba = []\n",
    "    y_test_true = []\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for sub_data in data_loader:\n",
    "            sub_data = sub_data.to(device)\n",
    "            out = model(sub_data.x, sub_data.edge_index)\n",
    "            y_test_pred.append(out[sub_data.test_mask].argmax(dim=1).cpu())\n",
    "            y_test_pred_proba.append(out[sub_data.test_mask].softmax(dim=1).cpu())\n",
    "            y_test_true.append(sub_data.y[sub_data.test_mask].cpu())\n",
    "\n",
    "    y_test_pred = torch.cat(y_test_pred, dim=0).numpy()\n",
    "    y_test_pred_proba = torch.cat(y_test_pred_proba, dim=0).numpy()\n",
    "    y_test_true = torch.cat(y_test_true, dim=0).numpy()\n",
    "    performance = performance_per_class(y_test_true, y_test_pred, y_test_pred_proba)\n",
    "    return performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GNN Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Sequential, Dropout, Linear\n",
    "from torch_geometric.nn import GCNConv, SAGEConv, GATConv\n",
    "from torch_geometric.nn.conv import MessagePassing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_activation(activation: str) -> torch.nn.Module:\n",
    "    if activation == 'relu':\n",
    "        return torch.nn.ReLU()\n",
    "    elif activation == 'sigmoid':\n",
    "        return torch.nn.Sigmoid()\n",
    "    elif activation == 'tanh':\n",
    "        return torch.nn.Tanh()\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported activation function. Choose from 'relu', 'sigmoid', or 'tanh'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a base GNN class for all GNN variants\n",
    "class BaseGNN(torch.nn.Module):\n",
    "    def __init__(self, input_dim: int, n_class: int, hidden_dims: list, dropout: float, activation: str, conv_layer: MessagePassing):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(888)\n",
    "        \n",
    "        activation_fn = build_activation(activation)\n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        \n",
    "        # Add convolutional layers\n",
    "        for h_dim in hidden_dims:\n",
    "            layers.append(conv_layer(prev_dim, h_dim))  # Conv layer passed as argument\n",
    "            layers.append(activation_fn)\n",
    "            layers.append(Dropout(dropout))\n",
    "            prev_dim = h_dim\n",
    "        \n",
    "        # Append classifier layer\n",
    "        layers.append(Linear(prev_dim, n_class))\n",
    "        self.network = Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, edge_index: torch.Tensor) -> torch.Tensor:\n",
    "        for layer in self.network:\n",
    "            if isinstance(layer, (GCNConv, SAGEConv, GATConv)):  # Conv layers need edge_index\n",
    "                x = layer(x, edge_index)\n",
    "            else:\n",
    "                x = layer(x)  # Other layers only need x\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(BaseGNN):\n",
    "    def __init__(self, input_dim: int, n_class: int, hidden_dims: list, dropout: float, activation: str):\n",
    "        super().__init__(input_dim, n_class, hidden_dims, dropout, activation, conv_layer=GCNConv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphSAGE(BaseGNN):\n",
    "    def __init__(self, input_dim: int, n_class: int, hidden_dims: list, dropout: float, activation: str, aggr: str):\n",
    "        super().__init__(input_dim, n_class, hidden_dims, dropout, activation, \n",
    "                         conv_layer=lambda in_dim, out_dim: SAGEConv(in_dim, out_dim, aggr=aggr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAT(BaseGNN):\n",
    "    def __init__(self, input_dim: int, n_class: int, hidden_dims: list, dropout: float, activation: str, heads: int):\n",
    "        conv_layer = lambda in_dim, out_dim: GATConv(in_dim, out_dim, heads=heads)\n",
    "        super().__init__(input_dim, n_class, hidden_dims, dropout, activation,conv_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_GNN_parameters = {\n",
    "    'hidden_dims': {'values': [[64, 32], [128, 64, 32]]},\n",
    "    'dropout': {'values': [0.1, 0.3, 0.5]},\n",
    "    'activation': {'values': ['relu', 'sigmoid', 'tanh']},\n",
    "    'optimizer': {'values': ['sgd', 'adam']},\n",
    "    'lr': {'values': [0.001, 0.01, 0.1]},\n",
    "    'n_class': {'value': data.num_classes},\n",
    "    'input_dim': {'value': data.num_features},\n",
    "    'graph_num_parts': {'value': 128},\n",
    "    'batch_size': {'value': 32},\n",
    "    'epochs': {'value': 20},\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GraphSage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_GraphSAGE(config: Config) -> torch.nn.Module:\n",
    "    model = GraphSAGE(config.input_dim, config.n_class, config.hidden_dims, config.dropout, \n",
    "                config.activation, config.aggr).to(device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GraphSAGE_parameters = copy.deepcopy(base_GNN_parameters)\n",
    "GraphSAGE_parameters['aggr'] = {'values': ['mean', 'max']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GraphSAGE_performance = evaluate_GNN(data, build_GraphSAGE, \"AKI_GNN_GraphSAGE\", GraphSAGE_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GraphSAGE_performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_GAT(config: Config) -> torch.nn.Module:\n",
    "    model = GAT(config.input_dim, config.n_class, config.hidden_dims, config.dropout, \n",
    "                config.activation, config.heads).to(device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAT_parameters = copy.deepcopy(base_GNN_parameters)\n",
    "GAT_parameters['heads'] = {'values': [2, 4, 8]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAT_performance = evaluate_GNN(data, build_GAT, \"AKI_GNN_GAT\", GAT_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAT_performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_GCN(config: Config) -> torch.nn.Module:\n",
    "    model = GCN(config.input_dim, config.n_class, config.hidden_dims, config.dropout, config.activation).to(device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GCN_parameters = copy.deepcopy(base_GNN_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GCN_performance = evaluate_GNN(data, build_GCN, \"AKI_GNN_GCN\", GCN_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GCN_performance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AKI_GNN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
