{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Union\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer, classification_report, f1_score, roc_auc_score, average_precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(41467, 69)\n"
     ]
    }
   ],
   "source": [
    "onset_df_pilot = pd.read_csv('/blue/yonghui.wu/lideyi/AKI_GNN/raw_data/norm_df_pilot.csv')\n",
    "print(onset_df_pilot.shape)\n",
    "n_class = len(np.unique(onset_df_pilot.AKI_TARGET))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = [col for col in onset_df_pilot.columns if (col != 'AKI_TARGET') and (col != 'TEST_SET')]\n",
    "X_train = onset_df_pilot.loc[onset_df_pilot['TEST_SET'] == 0, feature_columns].copy(deep=True)\n",
    "y_train = onset_df_pilot.loc[onset_df_pilot['TEST_SET'] == 0, 'AKI_TARGET'].copy(deep=True)\n",
    "X_test = onset_df_pilot.loc[onset_df_pilot['TEST_SET'] == 1, feature_columns].copy(deep=True)\n",
    "y_test = onset_df_pilot.loc[onset_df_pilot['TEST_SET'] == 1, 'AKI_TARGET'].copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23485, 67) (5872, 67) (12110, 67)\n"
     ]
    }
   ],
   "source": [
    "# split 20% of training set as validation set\n",
    "X_train, X_val = X_train.iloc[:int(0.8 * len(X_train))], X_train.iloc[int(0.8 * len(X_train)):]\n",
    "y_train, y_val = y_train.iloc[:int(0.8 * len(y_train))], y_train.iloc[int(0.8 * len(y_train)):]\n",
    "assert len(X_train) + len(X_val) + len(X_test) == len(onset_df_pilot)\n",
    "assert len(y_train) + len(y_val) + len(y_test) == len(onset_df_pilot)\n",
    "print(X_train.shape, X_val.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF = RandomForestClassifier(random_state=42)\n",
    "RF_parameters = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [3, 6, 10],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_model(model, parameters: dict, X_train: Union[np.ndarray, pd.Series], y_train: Union[np.ndarray, pd.Series],\n",
    "                             X_test: Union[np.ndarray, pd.Series], y_test: Union[np.ndarray, pd.Series]):\n",
    "    best_model = train_model(model, parameters, X_train, y_train)\n",
    "    model_performance = test_model(best_model, X_test, y_test)\n",
    "    return model_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, parameters: dict, X_train: Union[np.ndarray, pd.Series], y_train: Union[np.ndarray, pd.Series]):\n",
    "    macro_f1_scorer = make_scorer(f1_score, average='macro') \n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=model,\n",
    "        param_grid=parameters,\n",
    "        cv=5,\n",
    "        scoring=macro_f1_scorer,\n",
    "        n_jobs=-1,\n",
    "        verbose=1,\n",
    "        )\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    best_model = grid_search.best_estimator_\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(best_model, X_test: Union[np.ndarray, pd.Series], y_test: Union[np.ndarray, pd.Series]):\n",
    "    y_test_pred = best_model.predict(X_test)\n",
    "    y_test_pred_prob = best_model.predict_proba(X_test)\n",
    "    performance = individual_class_performance(y_test, y_test_pred, y_test_pred_prob)\n",
    "    return performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def individual_class_performance(y_test: Union[np.ndarray, pd.Series], y_test_pred: Union[np.ndarray, pd.Series], \n",
    "    y_test_pred_prob: Union[np.ndarray, pd.Series]) -> pd.DataFrame:\n",
    "    \n",
    "    n_class = len(np.unique(y_test))\n",
    "    # Generate the classification report as a dictionary\n",
    "    report_dict = classification_report(y_test, y_test_pred, output_dict=True)\n",
    "\n",
    "    # Convert to a pandas DataFrame for better formatting\n",
    "    metrics_table = pd.DataFrame(report_dict).transpose()\n",
    "\n",
    "    # Extract only the classes (0, 1, 2, 3) and metrics\n",
    "    class_metrics = metrics_table.loc[[str(c) for c in range(n_class)], ['precision', 'recall', 'f1-score']]\n",
    "    \n",
    "    # add AUROC and AUPRC\n",
    "    AUROC = []\n",
    "    AUPRC = []\n",
    "    for i in range(n_class):\n",
    "        y_true_binary = (y_test == i).astype(int)\n",
    "        y_proba_class = y_test_pred_prob[:, i]\n",
    "        # compute AUROC\n",
    "        AUROC.append(roc_auc_score(y_true_binary, y_proba_class))\n",
    "        # compute AUPRC\n",
    "        AUPRC.append(average_precision_score(y_true_binary, y_proba_class))\n",
    "        \n",
    "    # add to the class_metrics DataFrame\n",
    "    class_metrics['AUROC'] = AUROC\n",
    "    class_metrics['AUPRC'] = AUPRC\n",
    "    return class_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 81 candidates, totalling 405 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lideyi/conda/envs/AKI_GNN/lib/python3.9/site-packages/numpy/ma/core.py:2846: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n"
     ]
    }
   ],
   "source": [
    "RF_peformance = train_and_evaluate_model(RF, RF_parameters, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   precision    recall  f1-score     AUROC     AUPRC\n",
      "0   0.913366  0.977627  0.944405  0.914026  0.979285\n",
      "1   0.537623  0.352647  0.425918  0.866828  0.481983\n",
      "2   0.621359  0.204473  0.307692  0.955601  0.472790\n",
      "3   0.785714  0.687500  0.733333  0.989371  0.750948\n"
     ]
    }
   ],
   "source": [
    "print(RF_peformance)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
