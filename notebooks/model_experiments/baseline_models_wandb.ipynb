{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import wandb\n",
    "import torch\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "# mute wandb outputs\n",
    "import os\n",
    "os.environ[\"WANDB_SILENT\"] = \"true\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " TODO:\n",
    " 1. Use temporal method to cut train, val and test ✅\n",
    " 2. Scale individually ✅\n",
    " 3. Simplify the code (including organizing all GNNs into a base class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# login wandb\n",
    "wandb.login(key=\"62d0c78e72de6dacd620fc6d13ebfecfa7ce68a1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "onset_df_pilot = pd.read_csv('/blue/yonghui.wu/lideyi/AKI_GNN/raw_data/norm_df_pilot.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = [col for col in onset_df_pilot.columns if col not in ['AKI_TARGET', 'TRAIN_SET', 'VAL_SET', 'TEST_SET']]\n",
    "X_train = onset_df_pilot.loc[onset_df_pilot['TRAIN_SET'] == 1, feature_columns].copy(deep=True).values\n",
    "y_train = onset_df_pilot.loc[onset_df_pilot['TRAIN_SET'] == 1, 'AKI_TARGET'].copy(deep=True).values\n",
    "X_val = onset_df_pilot.loc[onset_df_pilot['VAL_SET'] == 1, feature_columns].copy(deep=True).values\n",
    "y_val = onset_df_pilot.loc[onset_df_pilot['VAL_SET'] == 1, 'AKI_TARGET'].copy(deep=True).values\n",
    "X_test = onset_df_pilot.loc[onset_df_pilot['TEST_SET'] == 1, feature_columns].copy(deep=True).values\n",
    "y_test = onset_df_pilot.loc[onset_df_pilot['TEST_SET'] == 1, 'AKI_TARGET'].copy(deep=True).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_RF(X_train: np.array, y_train: np.array, X_val: np.array, y_val: np.array,\n",
    "                X_test: np.array, y_test:  np.array, wandb_project_name: str, \n",
    "                parameters: dict) -> pd.DataFrame:\n",
    "    sweep_config = build_sweep_config(parameters)\n",
    "    sweep_id = wandb.sweep(sweep_config, project = wandb_project_name)\n",
    "    sweep_func = lambda: train_RF(X_train = X_train, y_train = y_train, X_val = X_val, y_val = y_val, config = None)\n",
    "    wandb.agent(sweep_id, sweep_func)\n",
    "    performance = test_best_RF(X_train, y_train, X_test, y_test, sweep_id)\n",
    "    return performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_sweep_config(parameters: dict) -> dict:\n",
    "    sweep_config = {\n",
    "    'method': 'grid',\n",
    "    'metric': {'name': 'val_F1', 'goal': 'maximize'},\n",
    "    'parameters': parameters,\n",
    "    }\n",
    "    return sweep_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_RF(X_train: np.array, y_train: np.array, X_val: np.array, y_val: np.array, config = None) -> None:\n",
    "    # Initialize a new wandb run\n",
    "    with wandb.init(config=config):\n",
    "        config = wandb.config\n",
    "        RF = build_RF(config.max_depth, config.min_samples_leaf,\n",
    "                      config.min_samples_split, config.n_estimators)\n",
    "        RF.fit(X_train, y_train)\n",
    "        val_F1 = evaluate_on_val(X_val, y_val, RF)\n",
    "        \n",
    "        wandb.log({\"val_F1\": val_F1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_RF(max_depth: int, min_samples_leaf: int, \n",
    "             min_samples_split: int, n_estimators: int) -> RandomForestClassifier:\n",
    "    \n",
    "    RF = RandomForestClassifier(max_depth=max_depth, \n",
    "                                min_samples_leaf=min_samples_leaf, \n",
    "                                min_samples_split=min_samples_split,\n",
    "                                n_estimators=n_estimators)\n",
    "    return RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_on_val(X_val: np.array, y_val: np.array, RF: RandomForestClassifier) -> float:\n",
    "    y_pred = RF.predict(X_val)\n",
    "    val_F1 = f1_score(y_val, y_pred, average='macro')\n",
    "    return val_F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_best_RF(X_train: np.array, y_train: np.array, \n",
    "                 X_test: np.array, y_test: np.array, sweep_id: str) -> pd.DataFrame:\n",
    "    best_config = fetch_best_config(sweep_id)\n",
    "    best_RF = build_RF(best_config['max_depth'], best_config['min_samples_leaf'], \n",
    "                    best_config['min_samples_split'], best_config['n_estimators'])\n",
    "    best_RF.fit(X_train, y_train)\n",
    "    best_RF_performance = test_RF(X_test, y_test, best_RF)\n",
    "    return best_RF_performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_best_config(sweep_id: str) -> dict:\n",
    "    # Authenticate with W&B\n",
    "    api = wandb.Api()\n",
    "    sweep = api.sweep(sweep_id)\n",
    "    runs = sweep.runs\n",
    "    \n",
    "    # Find the best run\n",
    "    best_run = max(runs, key=lambda run: run.summary.get(\"val_F1\", float(\"-inf\")))\n",
    "    best_hyperparams = best_run.config\n",
    "    return best_hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(os.path.abspath(\"/home/lideyi/AKI_GNN/notebooks/utils\"))\n",
    "from metrics import performance_per_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_RF(X_test: np.array, y_test: np.array, best_model: RandomForestClassifier) -> dict:\n",
    "    y_test_pred = best_model.predict(X_test)\n",
    "    y_test_pred_prob = best_model.predict_proba(X_test)\n",
    "    performance = performance_per_class(y_test, y_test_pred, y_test_pred_prob)\n",
    "    return performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: ccnpid7f\n",
      "Sweep URL: https://wandb.ai/ericli/AKI_GNN_RF/sweeps/ccnpid7f\n"
     ]
    }
   ],
   "source": [
    "RF_parameters = {\n",
    "    'max_depth': {'values': [3, 5, 10]},\n",
    "    'min_samples_leaf': {'values': [1, 2, 4]},\n",
    "    'min_samples_split': {'values': [2, 5, 10]},\n",
    "    'n_estimators': {'values': [50, 100, 200]}\n",
    "}\n",
    "RF_performance = evaluate_RF(X_train, y_train, X_val, y_val, X_test, y_test, \"AKI_GNN_RF\", RF_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-laryer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_MLP(X_train: np.array, y_train: np.array, X_val: np.array, y_val: np.array,\n",
    "                X_test: np.array, y_test:  np.array, wandb_project_name: str, \n",
    "                parameters: dict) -> pd.DataFrame:\n",
    "    sweep_config = build_sweep_config(parameters)\n",
    "    sweep_id = wandb.sweep(sweep_config, project = wandb_project_name)\n",
    "    sweep_func = lambda: train_MLP_main(X_train = X_train, y_train = y_train, X_val = X_val, y_val = y_val, config = None)\n",
    "    wandb.agent(sweep_id, sweep_func)\n",
    "    performance = test_best_MLP(X_train, y_train, X_test, y_test, sweep_id)\n",
    "    return performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_MLP_main(X_train: np.array, y_train: np.array, X_val: np.array, y_val: np.array, config = None) -> None:\n",
    "    # Initialize a new wandb run\n",
    "    with wandb.init(config=config):\n",
    "        config = wandb.config\n",
    "        model = build_MLP(config.input_dim, config.n_class, config.hidden_dims, \n",
    "                        config.dropout, config.activation)\n",
    "        optimizer = build_optimizer(model, config.optimizer, config.lr)\n",
    "        train_loader = build_dataloader(X_train, y_train, config.batch_size, shuffle=True)\n",
    "        val_loader = build_dataloader(X_val, y_val, config.batch_size, shuffle=False)\n",
    "        train_MLP(model, config.epochs, optimizer, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_MLP(model: torch.nn.Module, epochs: int, optimizer: torch.optim.Optimizer, \n",
    "              train_loader: torch.utils.data.DataLoader, val_loader: torch.utils.data.DataLoader,\n",
    "              log: bool = True) -> None:\n",
    "    for _ in range(epochs):\n",
    "        train_F1, avg_loss_train = train_epoch(model, optimizer, train_loader)\n",
    "        if log:\n",
    "            wandb.log({\"train_loss\": avg_loss_train, \"train_F1\": train_F1})\n",
    "        if val_loader != None:\n",
    "            val_F1, avg_loss_val = val_epoch(model, val_loader)\n",
    "            if log:\n",
    "                wandb.log({\"val_loss\": avg_loss_val, \"val_F1\": val_F1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_MLP(input_dim: int, n_class: int, hidden_dims: list, dropout: float, activation: str) -> torch.nn.Module:\n",
    "    return MLP(input_dim, n_class, hidden_dims, dropout, activation).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, input_dim: int, n_class: int, hidden_dims: list, dropout: float, activation: str):\n",
    "        super(MLP, self).__init__()\n",
    "        torch.random.manual_seed(888)\n",
    "        # Define the activation function\n",
    "        if activation == 'relu':\n",
    "            activation_fn = torch.nn.ReLU()\n",
    "        elif activation == 'sigmoid':\n",
    "            activation_fn = torch.nn.Sigmoid()\n",
    "        elif activation == 'tanh':\n",
    "            activation_fn = torch.nn.Tanh()\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported activation function. Choose from 'relu', 'sigmoid', or 'tanh'.\")\n",
    "        \n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        \n",
    "        for h_dim in hidden_dims:\n",
    "            layers.append(torch.nn.Linear(prev_dim, h_dim))\n",
    "            layers.append(activation_fn)\n",
    "            layers.append(torch.nn.Dropout(dropout))\n",
    "            prev_dim = h_dim\n",
    "            \n",
    "        # append classifier layer\n",
    "        layers.append(torch.nn.Linear(prev_dim, n_class))\n",
    "        self.network = torch.nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataloader(X: np.array, y: np.array, batch_size: int, shuffle: bool) -> torch.utils.data.DataLoader:\n",
    "    dataset = torch.utils.data.TensorDataset(torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.long))\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_optimizer(model: torch.nn.Module, optimizer: str, lr: float) -> torch.optim.Optimizer:\n",
    "    if optimizer == \"sgd\":\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "    elif optimizer == \"adam\":\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model for one epoch\n",
    "def train_epoch(model: torch.nn.Module, optimizer: torch.optim.Optimizer, \n",
    "                train_loader: torch.utils.data.DataLoader) -> tuple[float, float]:\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        y_pred_batch = model(X_batch)\n",
    "        loss = torch.nn.functional.cross_entropy(y_pred_batch, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        y_true.extend(y_batch.cpu().numpy())\n",
    "        y_pred.extend(y_pred_batch.argmax(dim=1).detach().cpu().numpy())\n",
    "    macro_F1 = f1_score(y_true, y_pred, average='macro')\n",
    "    return macro_F1, total_loss / len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validate the model on the validation set, return the macro-F1\n",
    "def val_epoch(model: torch.nn.Module, val_loader: torch.utils.data.DataLoader) -> tuple[float, float]:\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            y_pred_batch = model(X_batch)\n",
    "            loss = torch.nn.functional.cross_entropy(y_pred_batch, y_batch)\n",
    "            total_loss += loss.item()\n",
    "            y_true.extend(y_batch.cpu().numpy())\n",
    "            y_pred.extend(y_pred_batch.argmax(dim=1).detach().cpu().numpy())\n",
    "    macro_F1 = f1_score(y_true, y_pred, average='macro')\n",
    "    return macro_F1, total_loss / len(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_best_MLP(X_train: np.array, y_train: np.array, \n",
    "                 X_test: np.array, y_test: np.array, sweep_id: str) -> pd.DataFrame:\n",
    "    best_config = fetch_best_config(sweep_id)\n",
    "    best_model = build_MLP(best_config['input_dim'], best_config['n_class'], best_config['hidden_dims'], \n",
    "                    best_config['dropout'], best_config['activation'])\n",
    "    optimizer = build_optimizer(best_model, best_config['optimizer'], best_config['lr'])\n",
    "    train_loader = build_dataloader(X_train, y_train, best_config['batch_size'], shuffle=True)\n",
    "    test_loader = build_dataloader(X_test, y_test, best_config['batch_size'], shuffle=False)\n",
    "    train_MLP(best_model, best_config['epochs'], optimizer, train_loader, None, log=False)\n",
    "    performance = test_MLP(test_loader, best_model)\n",
    "    return performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_MLP(test_loader: torch.utils.data.DataLoader, best_model: torch.nn.Module) -> pd.DataFrame:\n",
    "    best_model.eval()\n",
    "    y_true, y_pred, y_pred_proba = [], [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            y_pred_batch = best_model(X_batch)\n",
    "            y_true.extend(y_batch.cpu().numpy())\n",
    "            y_pred.extend(y_pred_batch.argmax(dim=1).detach().cpu().numpy())\n",
    "            # get y_pred_proba\n",
    "            y_pred_proba_batch = torch.nn.functional.softmax(y_pred_batch, dim=1).cpu().numpy()\n",
    "            y_pred_proba.append(y_pred_proba_batch)\n",
    "             \n",
    "    y_true, y_pred, y_pred_proba = np.array(y_true), np.array(y_pred), np.concatenate(y_pred_proba)\n",
    "    performance = performance_per_class(y_true, y_pred, y_pred_proba)\n",
    "    return performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: n6aa7cc7\n",
      "Sweep URL: https://wandb.ai/ericli/AKI_GNN_MLP/sweeps/n6aa7cc7\n"
     ]
    }
   ],
   "source": [
    "MLP_parameters = {\n",
    "    'hidden_dims': {'values': [[64, 32, 16], [128, 64, 32, 16]]},\n",
    "    'dropout': {'values': [0.1, 0.3, 0.5]},\n",
    "    'activation': {'values': ['relu', 'sigmoid', 'tanh']},\n",
    "    'optimizer': {'values': ['sgd', 'adam']},\n",
    "    'lr': {'values': [0.001, 0.01, 0.1]},\n",
    "    'n_class': {'value': len(np.unique(y_train))},\n",
    "    'input_dim': {'value': X_train.shape[1]},\n",
    "    'batch_size': {'value': 64},\n",
    "    'epochs': {'value': 20},\n",
    "}\n",
    "\n",
    "MLP_performacne = evaluate_MLP(X_train, y_train, X_val, y_val, X_test, y_test, \"AKI_GNN_MLP\", MLP_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLP_performacne"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AKI_GNN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
